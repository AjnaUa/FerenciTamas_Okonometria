[["index.html", "Ökonometria Elszó", " Ökonometria Ferenci Tamás, 2021. február 1. Elszó Az ökonometriai a társadalmi-gazdasági jelenségek számszersített, empirikus  azaz tapasztalati, tényadatokon alapuló  vizsgálatának, modellezésének a tudománya Az ökonometriai a társadalmi-gazdasági jelenségek számszersített, empirikus  azaz tapasztalati, tényadatokon alapuló  vizsgálatának, modellezésének a tudománya. Szemben azzal, amit esetleg a név sugallhat, közel sem csak közgazdászoknak fontos: szociológusok, társadalomkutatók számára ugyanúgy alapvet az ökonometria ismerete. St, maguk a módszerei még ennél is szélesebb körben, ahol empirikus adatok kezelésére szükség van, biostatisztikától a pszichometrián át az agrometriáig, felhasználhatóak. (Ahogy szokták is mondani: a ,,statisztika egy.) Tárgyalás matematikai részletek nélkül, inkább sok területet érintve (de elméletileg precízen) Ez a jegyzet ezeket a módszereket tárgyalja, az alapoktól kezdve. Nem célja mély matematikai részletek tárgyalása (noha a korszer ökonometriára ez nagyon is jellemz), inkább a módszerek, alkalmazási területek, és eszközök sokféleségét kívánja bemutatni. Az elméleti szigorból azonban nem enged. Számítógépes munka bemutatásához R statisztikai környezet alatti illusztrációk Manapság ökonometria elképzelhetetlen számítógépes támogatás nélkül. Bár a jegyzetnek nem kifejezett célja ennek megtanítása, de igyekszik hozzá minden segítséget megadni: valamennyi eredmény elállítását is bemutatja a manapság egyre népszerbb R statisztikai programcsomag alatt. (Az R általános statisztikai programcsomag, és bár klasszikus orientációja nem kimondottan ökonometriai, erre a célra is egyre jobban használható, kitn általános tulajdonságai és a megjelen kiegészít csomagok sokaságának köszönheten.) Minden visszajelzést örömmel veszek a tamas.ferenci@medstat.hu email-címen A jegyzettel kapcsolatban minden visszajelzést, véleményt, kritikát a lehet legnagyobb örömmel veszek a tamas.ferenci@medstat.hu email-címen! A jegyzet weboldala (oktatási segédanyagokkal, technikai információkkal) a https://github.com/tamas-ferenci/FerenciTamas_Okonometria címen érhet el. "],["út-a-többváltozós-regresszióhoz.html", "1 . fejezet Út a többváltozós regresszióhoz 1.1 Történetünk els szála: néhány motiváló példa 1.2 A példák tanulságai: az empirikus adatok elemzésének legnagyobb problémája 1.3 A confounding megoldásai: kísérlet és megfigyelés 1.4 Történetünk második szála: a regressziós modellek 1.5 Regresszió a sokaságban 1.6 A szálak összeérnek", " 1 . fejezet Út a többváltozós regresszióhoz \\(\\DeclareMathOperator*{\\argmin}{arg\\,min}\\) \\(\\DeclareMathOperator*{\\argmax}{arg\\,max}\\) \\(\\DeclareMathOperator*{\\rank}{rank}\\) \\(\\def\\uuline#1{\\underline{\\underline{#1}}}\\) Egy ilyen tudomány esetén az els feladat annak tisztázása, hogy egyáltalán mi az az ökonometria, mire szolgál, és mi szükség van rá a társadalmi-gazdasági jelenségek elemzése során. A kérdést két történetszálon fogjuk végigvezetni (persze mint minden valamirevaló kortás norvég regényben, a szálak végül össze fognak érni). 1.1 Történetünk els szála: néhány motiváló példa Elsként, ahelyett, hogy rögtön a definíciókra térnénk, talán érdemesebb pár példát átbeszélni, melyek már mutatni fogják az ökonometriai vizsgálatok f problémáit. 1.1.1 Hogyan hat az osztálylétszám a tanulók teljesítményére? Kalifornia, 1999: 420 iskolai körzet adatait gyjtik be A jobb tanár:diák arányú (kisebb létszámú) osztályokban jobb a teljesítmény A közoktatásokkal kapcsolatos vizsgálatok egyik klasszikus kérdése, hogy az osztálylétszám hogyan hat a tanulók teljesítményére. Sokan amellett érvelnek, hogy a kisebb létszámú osztályokban több tanári figyelem jut egy diákra, így a tanulók teljesítménye jobb lesz. De vajon tényleg így van? E kérdésre számos módon válaszolhatunk: felállíthatunk elméleti modelleket, papíron és ceruzával, készíthetünk interjúkat szakértkkel, vizsgálhatunk analóg helyzeteket más területekrl stb. Mi azonban a továbbiakban egy módszerrel fogunk foglalkozni: ha empirikusan igyekszünk válaszolni a kérdésre. Empirikusan, annyi mint a tapasztalatok alapján, tehát való életbeli tényadatok begyjtévél. Elvégre az osztálylétszámokra csak van valamilyen adatgyjtés, ha az országban futnak standardizált képességmér felmér-programok, akkor a tanulók teljesítményére is van adatunk  mi lenne, ha 1999-ben Kalifornia állam pontosan ezzel a kérdéssel szembesült. 420 iskolai körzetbl gyjtött adatokat, melyek  számos egyéb mellett  tartalmazták a tanulók és tanárok létszámát, valamint az elért teszteredményeket1. Az AER csomag CASchools néven tartalmazza a tényleges adatokat. Lássuk is akkor az eredményt! Íme a tanár:diák arányok és a teszteredmények szóródási diagramok szemléltetve: data(&quot;CASchools&quot;, package = &quot;AER&quot;) CASchools$tsratio &lt;- with(CASchools, teachers/students) CASchools$score &lt;- with(CASchools, (math + read)/2) ggplot(CASchools, aes(x = tsratio, y = score)) + geom_point() + labs(x = &quot;Tanár:diák arány&quot;, y = &quot;Teszteredmény&quot;) Az eredmények els ránézésre megersítik a sejtésünket: ha több tanár jut egy diákra (kisebbek az osztályok), akkor az jobb teszteredménnyel jár együtt. Nem túl ers az összefüggés, de azért egyértelmen létezik (vájtfülek kedvéért: \\(r=0.23\\), \\(p &lt;0.001\\)) és némi munkával még az is kihozható, hogy ezen eredmények szerint ha egy századdal megnöveljük a tanár:diák arányt, akkor várhatóan 8.38 ponttal fog javulni a tesztpontszám. Ezek az eredmények húsbavágóak. Ha lecsökkentjük az osztálylétszámokat, akkor több tanárt kell alkalmazni, több osztályt kell indítani, adott esetben több osztályteremre lesz szükség, de még az sem kizárt, hogy új iskolaépületre. Pontosan tudni kell tehát, hogy tényleg elérünk-e ezzel valamit. St, valójában ennél többrl van szó, azt is tudni kell, hogy mennyit érünk el ezzel, egész egyszeren azért, hogy költség-haszon mérleget lehessen csinálni: a tanárok bérét meg az iskolafelújítások árát megmondják a kontrollerek, de mit rakunk a mérleg másik serpenyjébe? Ehhez kell tudni a fenti számot, hogy a kettbl együtt meg tudjuk mondani: hány millió dollárt kell költenünk 1 pont teljesítményjavításra. Hogy aztán ez megéri-e, az természetesen már nem statisztikai kérdés, függ a rendelkezésre álló büdzsétl, az egyéb feladatoktól, de még a kormány értékválasztásától is, ám a statisztikának kell ezt, mint inputadatot szolgáltatni a döntéshez. De vajon biztos jól van így minden? Ha az ember elkezdi jobban nézni a problémát, esetleg ,,szociológiai szemmel is igyekszik ránézni, akkor hamar szöget üthet a fejében valami. És nem is kell Kaliforniáig menni, magyar, vagy akár még konkrétabban budapesti viszonylatban is ugyanúgy érzékelhet a probléma: ha veszünk kis átlaglétszámú osztályokat (sztereotipikusan mondjuk 2. kerület) és nagy átlaglétszámú osztályokat (sztereotipikusan mondjuk 8. kerület), akkor hihet, hogy az elbbiek teljesítménye jobb, na de álljunk meg egy pillanatra! Csak és kizárólag az osztálylétszám nagyságában térnek el ezek az osztályok egymástól?! Dehogy! Akkor meg honnan tudjuk, hogy a tapasztalt különbség tényleg az osztálylétszámbeli eltérés miatt van? A rövid válasz  sajnos  az, hogy sehonnan! És itt van a bökken: igaz, hogy a 2. kerületi osztályok kisebbek mint a 8. kerületiek, de egyúttal másban is eltérnek, az oda járó gyerekek szocioökonómiai háttere tendenciájában jobb, tanulásra motiválóbb otthoni környezetbl érkeztek, a szülk anyagilag is megengedhetik maguknak, hogy a gyermekeiket különórára járassák stb. E ponton viszont nagy baj van: innen kezdve fogalmunk sem lehet, hogy a tapasztalt különbség tényleg a kisebb osztálylétszám miatt van, vagy esetleg az osztálylétszámnak a világon semmi hatása nincs, csak egyszeren a kisebb osztályokba jobb szocioökonómiai helyzet diákok járnak és ez a valódi oka az ott tapasztalt jobb tesztpontszámnak? St! Innentl kezdve még akár az is elképzelhet, hogy a kisebb osztálylétszám igazából kifejezetten ront a teljesítményen önmagában, csak épp a kisebb osztályokba annyival jobb szociális helyzet diákok járnak, hogy az átfordítja a helyzetet. Valaki nem hiszi el, hogy ez még is lehetséges? Nos, gyártsunk egy egyszer szimulációt! Egyelre nem árulom el, hogy hogyan készítettem, de íme a végeredménye: Els ránézésre nagyjából megfelel a korábbi képnek. De nézzük csak meg jobban mi történik itt! (Mivel ezt saját kezleg generáltuk, így megtehetjük, hiszen tudjuk mi van a valóságban, az adatok hátterében). Az egyszerség kedvéért mondjuk, hogy a szocioökonómiai státusz egy bináris változó, ,,jó és ,,rossz a két lehetséges értéke. Nézzük a tanár:diák arány és a pontszám összefüggését a jó szocioökonómiai státuszú osztályok körében: Érdekes! Itt fordított a kapcsolat. Na de mi a helyzet a rossz szocioökonómiai státuszú osztályokban? Íme: Itt is negatív a kapcsolat! Ez meg hogy a viharban lehet?  kérdezhetné valaki. A rossz szociális helyzet csoporton belül is ront a kisebb osztálylétszám, a jó helyzeteken belül is ront, de összességében meg javít?! Egybl világosabb a helyzet, ha egy ábrán ábrázoljuk a kettt, csak eltér színekkel: Azonnal érthet, hogy mi történik, ha hozzávesszük a korábban mondottakat: a jobb tanár:diák arány igazából ront a helyzeten, de a jobb szociális helyzet diákok által alkotott osztályok egyszerre kisebbek és  szociális helyzetük, nem az osztálylétszám miatt!  jobb teljesítmények, és ez olyan ers effektus, hogy ha egyben vizsgáljuk az osztályokat, akkor a kisebb létszám rontó hatását átbillenti, hogy a kisebb osztályok a jobb szociális helyzetük miatt jobb teljesítmények. Így összeségében azt fogjuk látni, ahonnan indultunk: hogy a kisebb létszámú osztályok jobb teljesítmények. A végeredmény tehát: a kisebb osztálylétszám rontja a teljesítményt és a kisebb létszámú osztályok jobb teljesítmények. És ha valaki érti a fentieket, akkor azt is érti, hogy ebben a mondatban miért nincs semmi ellentmondás! 1.1.2 Csökkenti-e a korrupció mértékét a nk részvétele a politikában? TODO 1.1.3 Csalnak-e az orosz választásokon? TODO 1.1.4 További példák Hosszasan sorolhatóak a további, hasonló példák a társadalmi-gazdasági elemzések világából. Kommentár nélkül még néhány kérdés, érdemes mindegyiket a fenti példákból leszrdni kezdd tanulságok szemüvegén keresztül végiggondolni: Hogyan hat a munkanélküliség a GDP-re? Hogyan hat az államadósság a növekedésre? Return on education: mekkora az oktatás haszna, tehát, ha egy évvel többet tölt valaki az iskolapadban, az mennyivel növeli a fizetését? Létezik-e ,,cigánybnözés? Az ökonometria-eladás haszna: ha többet tölt a hallgató az öko eladáson, jobb jegyet kap-e emiatt, és ha igen, mennyivel? Milyen tényezk hatnak arra, hogy egy országban hány terrortámadás történik? Hogyan hat a rendri erk létszáma egy adott városban az ottani bnözési rátákra? Cégeknek adott továbbképzési támogatás hogyan hat a termelékenységre? (Igen, ezekre mind válaszolhatunk ökonometriai módszerekkel!) 1.2 A példák tanulságai: az empirikus adatok elemzésének legnagyobb problémája Valamilyen ok-okozati hatásra vagyunk kíváncsiak; a kauzalitás érdekel minket Számos vizsgálati módszer közül most az empirikus adatok elemzésével fogunk foglalkozni: tényadatokat gyjtünk be, és ebbl igyekszünk következtetni A mintázat most már látszik. Valamilyen tényez hatására vagyunk kíváncsiak, az okozati hatására, szép szóval: a kauzalitásra, ez mindegyik példa lelke. Úgy döntünk, hogy a kérdést empirikus adatok elemzésével igyekszünk megoldani, azaz való életbeli tényadatokat gyjtünk be. (A vizsgált tényezrl, a hatásról, esetleg egyéb fontos változókról.) Azért, hogy eldöntsük, hogy van-e okozati hatás (illetve, hogy lemérjük mekkora), csoportokat hasonlítunk össze, melyek eltérnek a vizsgált tényezben. Csak épp közben a vizsgált tényezbeli eltéréssel automatikusan együtt járnak egyéb tényezbeli eltérések, és innentl kezdve bajban vagyunk, mert ha találunk is különbséget a csoportok között, nem fogjuk tudni, hogy az mi miatt van: a vizsgált tényezbeli eltérés miatt, a vele együtt járó egyéb eltérések miatt, vagy esetleg ezek valamilyen keveréke miatt. Ezt a jelenséget, mely az empirikus adatok elemzésének legnagyobb problémája, hívják confoundingnak. (Angolul nagyon jó szó, amire nem sikerült hasonlóan találó magyar fordítást bevezetni. A confounding azt jelenti, hogy összemosódás, és csakugyan, az a probléma, hogy a vizsgált tényezbeli eltérés összemosódik egyéb tényezkbeli eltérésekkel.) Vegyük észre, hogy a confounding fellépéséhez az kellett, hogy létezzen olyan tényez, amire két dolog egyszerre igaz: együttmozog a vizsgált tényezvel (összefügg vele) és önmagában  azaz a vizsgált tényez minden értéke mellett  hat az eredményváltozóra. Akkor van confounding, ha ez a kett egyidejleg fennáll. Ha bármelyik nincs jelen, akkor nincs probléma. Ha a szociális helyzet összefügg ugyan az osztálylétszámmal, de nem hat a teljesítményre, akkor nincs baj: igaz, hogy a kisebb osztályok jobb szociális helyzetek, de ez nem befolyásolja a teljesítményt. Hasonlóképp, ha a szociális helyzet befolyásolja ugyan a teljesítményt, de nem függ ösze az osztálymérettel, akkor sincs gond: a kisebb osztályoknak nem tér el a szociális helyzete a nagyobbaktól. (Gondoljuk végig az összes többi példára is!) Azokat a változókat, amelyek ezt a két dolgot egyidejleg tudják, tehát a vizsgált tényezvel együttmozognak és az eredményváltozóra is hatnak, ilyen módon okozzák a confoundingot, szokás zavaró változónak, vagy confoundernek nevezni. A késbbiek szempontjából hasznos lesz ezt még másképp átfogalmazni. A probléma, hogy nem az érdekel minket, hogy egy osztály abban tér el, hogy kisebb a létszám, akkor ott jobb-e a teljesítmény, hanem az, hogy ha csak abban tér el, hogy kisebb a létszám, akkor ott jobb-e a teljesítmény. Ezt szokás ceteris paribus elvnek (,,minden mást változatlanul tartva) nevezni, ez a kulcs a kauzalitáshoz: az érdekel minket, hogy ha minden mást változatlanul tartva csak az osztálylétszám változik, akkor mi történik. A naiv elemzésben az osztálylétszám változásával együtt egyéb tényezk is változhatnak, így ebbl nem tudunk a kauzalitásra következtetni. Figyeljünk a szóhasználatra: azt mondhatjuk, hogy a kisebb osztálylétszám jobb teljesítménnyel jár együtt (korreláció), de azt nem, hogy a kisebb osztálylétszám jobb teljesítményt okoz (kauzalitás). Valaki esetleg azt mondhatja, hogy rendben, itt tényleg van valami módszertani gubanc, meg szép latin szavak2 de igazából ez csak az ilyen módszertani kérdéseken szöszmötöl kutatóknak érdekes, a lényeg, hogy ha kisebb az osztálylétszám, akkor ott jobb a teljesítmény, ennyi a fontos, és pont. Nem! Ez az érvelés teljesen fals, az hogy mi hat mire, nem tudományos szrszálhasogatás, hanem elsrend gyakorlati kérdés. Miért? A beavatkozás miatt! A valódi okozatiság felismerése ott válik kritikussá, ha beavatkozunk a rendszerbe, ha ugyanis rosszul állapítjuk meg az okozati kapcsolatok irányát, akkor ez teljesen félremehet. Például lecsökkentjük az osztálylétszámokat, adott esetben rengeteg pénzt elköltve, de ha a valódi oka a jobb teljesítménynek nem az osztálylétszám, hanem a jobb szociális helyzet, akkor ezzel semmit nem érünk el! St, mint a késbbi példa mutatja, adott esetben még kimondottan árthatunk is! Végezetül még egy megjegyzés. A confounding felismerése nem azt jelenti, hogy akkor igazából nincs hatás, végképp nem azt, hogy bizonyítottuk, hogy ellentétes irányú hatás van. Pusztán annyit jelent, hogy a confounding-gal terhelt adatok nagyon gyenge bizonyítékot jelentenek a hatás léte mellett. De ettl még lehet éppenséggel hatás!  csak az ilyen adatok nagyon kevéssé támasztják ezt alá. 1.3 A confounding megoldásai: kísérlet és megfigyelés Most, hogy alaposan kiveséztük a confounding problémáját, természetesen adódik a kérdés: na de mit tehetünk ez ellen? Azt könny lenne biztosítani, hogy a csoportok egy-két általunk megadott szempont szerint ne legyenek eltérek, de azt, hogy egyáltalán semmilyen szempont szerint ne térjenek el (kivéve persze az vizsgált tényezt), olyanok szerint sem, amikrl eszünkbe sem jut, hogy lehet bennük eltérés, csak egy módon lehet: ez a randomizálás. Ahogy a szó is sugallja, a randomizálás lényege, hogy a megfigyelési egységeket véletlenszeren sorsoljuk különböz csoportokba, majd ezeket a csoportokat tesszük ki a vizsgált tényeznek. Például pénzfeldobással döntjük el az óvoda végén minden egyes gyermekrl, hogy kis vagy nagy létszámú osztályba kerüljön. Ez azért jó, mert ilyen módon a két csoport között nem lesz szisztematikus különbség szocioökonómiai státuszban, de ami még fontosabb: semmilyen tényezben nem lesz szisztematikus különbség, a kék szemek vagy a balkezesek számában sem, hiszen a pénzfeldobás nyilván ezekre is érzéketlen. Ilyen módon a csoportok összehasonlíthatóak: ha találunk köztük különbséget a tanulmányi eredményben, az tényleg az osztálylétszámnak lesz betudható hiszen másban nincs szisztematikus különbség. A randomizálásnak egy baja van: akkor alkalmazható, ha a vizsgált tényezt tudjuk irányítani. (Hiszen nekünk kell az egyik csoportba sorsolt gyerekeket kis, a másikat nagy létszámú osztályba helyezni.) Azokat a kutatásokat, ahol a kutatást végzk tudják irányítani a vizsgált tényezt, kísérletes (experimentális) kutatásnak nevezzük. És itt értünk el a bökkenhöz: a társadalmi-gazdasági jelenségek vizsgálata az a terület, ahol tipikusan nem lehet kísérletet végezni. Aligha lehet gyerekeket pénzfeldobással sorsolni osztályokba, vagy országokban pénzfeldobással meghatározni, hogy mennyi n üljön a kormányban (Persze ez sincs kbe vésve. Néha lehet kísérletet csinálni, ahogy a választási megfigyelk példája is mutatja. Másik oldalról, például az orvostudományban sokszor lehet kísérletet csinálni, ez a jellemz új gyógyszerek bevezetésénél, ahol a vizsgálat során véletlenszeren kiválasztott alanyok kapnak gyógyszert, míg a többiek placebot, de ott is van olyan kérdés, ahol nem lehet kísérletet csinálni! Tipikusan ilyenek fordulnak el az epidemiológiában: a vörös hús rákkelt? Aligha lehet emberekkel pénzfeldobás alapján évtizedekig több vagy kevesebb vörös húst etetni Innentl a probléma ott is ugyanaz: ha a vörös húst evk között több a rákos, az nagyon gyenge bizonyíték, mert a több vörös húst fogyasztó emberek milliónyi egyéb dologban is eltérnek a kevesebb vörös húst fogyasztó emberektl a vörös hús fogyasztás mértékén túl  és mi van, ha ezek közül valami növeli a rákkockázatot?) Azokat a vizsgálatokat, ahol a kutatást végzk nem tudják befolyásolni a vizsgált tényezt, az alakul a maga rendje szerint, és a kutatók csak passzíve feljegyzik a történéseket küls szemlélként, megfigyeléses (obszervációs) vizsgálatnak nevezzük. A társadalmi-gazdasági elemzések során tehát szinte mindig ilyenekkel lesz dolgunk. Márpedig ezeknél mindig fejünk felett fog lebegni a confounding problémája. 1.4 Történetünk második szála: a regressziós modellek Folytassuk most valami  látszólag  teljesen más témával. Minden fenti példában volt egy változó, mely az eredménye volt a vizsgálatunknak, a kimenet szerepét játszotta, tehát aminek az alakulását le kívántuk írni (tesztpontszám, korrupció mértéke, szavazati arány stb.). A továbbiakban ezt eredményváltozónak (vagy függ változónak, angolul response) fogjuk hívni, jele \\(Y\\). Az els példánkban \\(Y=\\text{Teszteredmény}\\). Másrészrl voltak változók, adott esetben nem is egy, amikkel le akarjuk írni az eredményváltozó alakulását, amelyekrl azt mondjuk, hogy hatnak, vagy hathatnak az eredményváltozóra; ezek neve magyarázó változó (vagy független változó, angolul predictor). Ezekbl több is lehet (az els példában ilyen az osztálylétszám és a szocioökonómiai helyzet), jelöljük számukat \\(k\\)-val, és az egyes változókat \\(X_i\\)-vel (\\(i=1,2,\\ldots,k\\)). Az els példában \\(k=2\\) és \\(X_1=\\text{Tanár:diák arány}\\), \\(X_2=\\text{Szocioökonómiai státusz}\\). Összefoglalva, az eredményváltozó a vizsgált kimenet, a magyarázó változók az azt  potenciálisan  befolyásoló tényezk (tehát a fontos, vizsgált változók és a  potenciális  confounderek egyaránt). Az \\(X\\)-ek hatnak az \\(Y\\)-ra, vagy fordítva megfogalmazva, az \\(Y\\) függ az \\(X\\)-ektl  ragadjuk meg most ezt matematikailag. Szerencsére arra, hogy egy változó függ más változóktól, ismerünk egy jó matematikai objektumot, ez a függvény fogalma: \\[ Y=f\\left(X_1,X_2,\\ldots,X_k\\right) \\] A késbbiekben erre azt fogjuk mondani, hogy ez egy statisztikai modell. Ennek az általánosságával nehéz lenne vitatkozni, de egy baja mégis csak van. A f probléma, hogy a modell azt feltételezi, hogy az \\(Y\\) és az \\(X\\)-ek kapcsolata determinisztikus. Szinte teljesen mindegy is, hogy mi az \\(Y\\) és mik az \\(X\\)-ek, hogy mi a vizsgált probléma, a társadalmi-gazdasági jelenségek vizsgálata kapcsán lényegében általánosan kijelenthet, hogy ez irreális: bármilyen ügyesek vagyunk, soha az életben nem fogunk tudni determinisztikus modelleket alkotni társadalmi-gazdasági jelenségekre. (Aligha lehet olyan modellt alkotni, ami pontosan, hiba nélkül megmondja elre, hogy egy osztály milyen pontszámot fog elérni, vagy, hogy egy választáson pontosan hány szavazat érkezik egy pártra.) Ez legfeljebb középiskolás fizikában mködik, a társadalmi-gazdasági jelenségekben szinte kizárt, hogy függvényszer módon meghatározzák a magyarázó változók az eredményváltozót. Hiszen lesznek változók amiket nem ismerünk, rosszul mérünk, rosszul veszünk figyelembe, az, hogy egy gyerek hány pontot ír egy teszten, mindig függ a mi közelítési szintünk ténylegesen véletlen dolgoktól stb. A A valódi modell tehát sztochasztikus kell legyen: \\[ Y=f\\left(X_1,X_2,\\ldots,X_k\\right)+\\varepsilon \\] Itt \\(\\varepsilon\\) jelzi a fentiekbl fakadó bizonytalanságot, a neve: hibatag. Rövid jelölésként az \\(X\\)-eket gyakran egy vektorba vonjuk össze: \\(Y=f\\left(\\underline{X}\\right)+\\varepsilon\\). Az így kapott modellünk már egy teljes érték statisztikai (ökonometriai) modell! Az ilyen \\(f\\)-et hívjuk (sokasági) regressziófüggvénynek. Már most is fontos, hogy lássuk, hogy az \\(f\\)-nek van egy nagyon is földhözragadt értelmezése: ezt kell használni, ha szeretném ,,megtippelni \\(Y\\) értékét \\(X\\)-ek ismeretében. Hogy ez miért lesz fontos, azt majd késbb fogjuk látni, de a feladat így is értelmes: ha ismerem egy osztály tanár:diák arányát és a szociális helyzetet, akkor ezek alapján mit mondhatok a teszteredményérl. Ha ezek tényleg hatnak rá, akkor valamit mondhatok, ezt fejezi ki az \\(f\\)-es rész, \\(\\varepsilon\\) pedig azt, amit nem tudok ezek alapján megmondani, azaz ettl lesz ez csak tipp: mindenképp kell számolnom azzal, hogy a valóság ettl a fent említett okok miatt eltér. Ez az egyenlet egy sokasági modell: azt írja le, hogy a valóság hogyan mködik. Pontosan ugyanaz a helyzet, mint bármilyen következtet statisztikai kurzus alapjainál: van a sokaság, amit eloszlásokkal, valószínségszámítási eszközökkel írunk le, de a tényleges vizsgálatokban mi sem ismerjük. (Tehát nem tudjuk, hogy ezek az eloszlások milyenek.) Ahhoz, hogy megismerjük veszünk egy mintát, ennek a kezeléséhez már statisztika kell, aminek a feladat épp az lesz, hogy következtessünk a sokaságra. Most is hasonló a helyzet: mi sem tudhatjuk, hogy milyen eloszlása van a teszteredményeknek, csak van egy 420 elem mintánk rá nézve; és hasonlóan a többi változóval. Most azonban egy pillanatig leszünk valszámos emberek: ne tördjünk azzal a problémával, hogy a sokaságot igazából nem ismerhetjük, játsszuk azt, hogy ismerjük (tudjuk mik ezek az eloszlások), és vizsgáljuk meg, hogy ebbl mire jutunk! Ugyanúgy mint a következtet statisztikánál, ez nagyon hasznos lesz majd késbb, a számunkra igazán érdekes  statisztikai  feladat megoldásánál is. A nem-kísérleti jelleg miatt az az értelmes modell, ha mind az eredményváltozót, mind a magyarázó változókat  és így persze \\(\\varepsilon\\)-t is  valószínségi változónak vesszük. (Ezért használtam eddig is nagy betket!) Bizonyos egyszersített tárgyalások úgy tekintik, mintha az \\(X\\)-ek nem valószínségi változók lennének, hanem rögzített értékek. Ez a kísérletek világában rendben lehet, ahol mi be tudjuk állítani az \\(X\\)-ek értékét, de ökonometriában, a társadalmi-gazdasági elemzések világában még közelít feltevésként is értelmetlen. 1.5 Regresszió a sokaságban Elsként tehát le kell írnunk a sokaságot: valszámos emberek leszünk, és úgy vesszük mintha ismernénk a sokaságot. Mit jelent ez, mit is ismerünk pontosan? Nem csak \\(Y\\) és \\(X_1,X_2,\\ldots,X_k\\) eloszlásait (külön-külön), hanem az együttes eloszlásukat is! Ekkor tudunk mindent ezekrl (valószínségszámítási értelemben). Ezt úgy kell elképzelnünk, mint egy \\(k+1\\) dimenziós teret: minden pont egy adott magyarázó- és eredményváltozó-kombináció. E fölött értelmezve van egy eloszlás, ami azt mutatja, hogy ha mintát veszünk ebbl az eloszlásból, akkor milyen valószínséggel esünk az adott pont kis környékére. \\(k+1\\) dimenziós terekben a legtöbb ember relatíve rosszul tájékozódik, úgyhogy ábrázoljunk egy olyan együttes eloszlást, amikor még átlátható a dolog! Ez egy kétváltozós eloszlás együttes srségfüggvénye; itt az egyik változó játsza a magyarázó-, a másik az eredményváltozó szerepét. A mintavétel ebbl az eloszlásból azt jelenti, hogy kiveszünk egy iskolát (tehát tanár:diák arányt és teszteredményt egyszerre!); ahol magasan fut a srségfüggvény, arról a környékrl gyakran veszünk ki, ahol alacsonyan, ott ritkábban. A hiba mibenléte is jól érthet errl az ábráról: ha kiválasztunk egy adott konkrét \\(X_1\\)-et, ahhoz csak egyetlen \\(f\\left(X_1\\right)\\)-et adhatunk, mégis \\(Y\\) minden értéket felvehet, tehát lehetetlen, hogy ne hibázzunk. (Csak egyetlen egy pont lesz a végtelen sok közül, ahol nem hibázunk.) Persze \\(f\\left(X_1\\right)\\)-et majd pont úgy lesz célszer megválasztani, hogy oda rakjuk, ahol \\(Y\\) gyakran elfordul, hogy a gyakran elforduló esetekben hibázzunk picit, és csak a ritkábbakban nagyobbat  de errl majd kicsit késbb. Elárulom, hogy ez az eloszlás többváltozós normális (késbb ennek majd jelentsége lesz), \\(\\boldsymbol{\\mu}=\\begin{pmatrix} 654 \\\\ 0.0514 \\end{pmatrix}\\) várhatóérték-vektorral és \\(\\mathbf{C}=\\begin{pmatrix} 19,\\!1^2 &amp; 0,\\!23 \\cdot 19,\\!1 \\cdot 0,\\!00515 \\\\ 0,\\!23 \\cdot 19,\\!1 \\cdot 0,\\!00515 &amp; 0,\\!00515^2 \\end{pmatrix}\\) kovariancia-mátrixszal3. Sajnos ez az ábrázolás nehezen érzékelhet (pláne, ha nem interaktívan nézzük, és nincs módunk forgatni), jobban járunk, ha így rajzoljuk ki: Ez ugyanaz mint a fenti srségfüggvény, de ,,szintvonalakkal leírva (azaz különböz \\(z\\) magasságokban elmetszettük a srségfüggvényt és a kapott metszeteket ábrázoltuk). Belátható, hogy többváltozós normális esetén ezek mindig ellipszisek4. Ezt az ábrázolást szokás ,,contour plot-nak nevezni, elnye, hogy  a háromdimenziós érzékeltetéssel szemben  nem érzékeny a nézpont megválasztására, részek nem takarnak ki másokat stb. (Ám cserében nyilván információ-vesztéssel jár, ami azzal arányos, hogy milyen srn képezzük a metszeteket.) Térjünk most vissza az alapkérdésünkre! Úgy vesszük, hogy ez az eloszlás adott, és le akarjuk írni mint \\(Y=f\\left(\\underline{X}\\right)+\\varepsilon\\); de vajon mi \\(f\\)-re a legjobb választás? Persze egy ilyen kérdést hallva azonnal vissza kell kérdezni: mi a jóság mérszáma? Hiszen csak ennek ismeretében mondható meg, hogy mi az optimális sokasági regressziófüggvény. Mivel az \\(\\varepsilon\\) hibát fejez ki, így azzal valószínleg kevesen vitatkoznának, hogy az a legjobb \\(f\\), amely mellett a hiba a legkisebb. Igen ám, de mi az, hogy a hiba a ,,legkisebb? Ez nem olyan nyilvánvaló, ennek megértéséhez beszéljünk egy picit a hibáról. A helyzetet a fenti példán úgy kell elképzelnünk, hogy behúzzuk a \\(f\\left(X_1\\right)\\) függvényt; ez olyan mintha rajzolnánk egy görbét az \\(X_1-Y\\) síkra. Ez után végigmegyünk a sík minden pontján, és megnézzük ott mekkora a hiba: mennyire van távol \\(Y\\) az \\(f\\left(X_1\\right)\\)-tl; ez pedig akkora súllyal fog szerepet játszani a hiba eloszlásában, amilyen magasan fut az adott ponton a srségfüggvény. Mindezek természetesen ugyanígy mködnek az általános, \\(k+1\\) dimenziós esetben is. A hibának tehát egy eloszlása van, így nem egyértelm, hogy mikor a ,,legkisebb. Két dolgot kell tennünk, az egyik választás egyértelm, de a másik már inkább döntés kérdése. Az els, hogy a hiba helyett annak \\(\\mathbb{E}\\varepsilon\\) várható értékét tekintjük. Ez jó, mert így a valószínségi változóból rögtön egy számot kapunk, amire pedig azonnal jobban értjük, hogy mit jelent az, hogy legyen a legkisebb. De igazán azért jó, mert ha összekombináljuk a várható érték fogalmát az elbbi bekezdés végén mondottakkal, akkor látjuk, hogy ez egy nagyon logikus dolgot mond: azt, hogy ott kevésbé számít a hibázás, ahová egyébként is ritkán esünk, és ott számít jobban a hibázás, ami gyakran elfordul! Azonban még nem végeztük. Ha meggondoljuk, akkor rögtön látjuk, hogy \\(\\mathbb{E}\\varepsilon\\) még nem lesz jó: a hiba lehet negatív is és pozitív is, de mi5 nem mondhatjuk azt, hogy ha egyszer 10-zel fölé lttünk, egyszer meg 10-zel alá, akkor tökéletesek voltunk. Magyarán: meg kell szabadulni az eljeltl. Itt már van választási lehetségünk, hogy mit teszünk, most döntsünk úgy (és jelen jegyzet túlnyomó többségében ezt adottságnak fogjuk venni), hogy négyzetre emeléssel szabadulunk meg az eljeltl, hiszen a négyzetre emelés függvény tulajdonságai nagyon kellemesek. Így tehát a megoldandó feladat: \\[ \\argmin_f \\mathbb{E}\\left[Y-f\\left(\\underline{X}\\right)\\right]^2 \\] Ez els ránézésre nagyon is ijeszten néz ki: optimalizációs feladat  az összes létez függvény terében?! Mert azt még érti az ember, hogy \\(x\\) felveszi az összes lehetséges valós számot, és mikor lesz \\(f\\left(x\\right)\\) minimális, na de mi az, hogy valami felveszi az összes létez (\\(k\\)-változós) függvényt? Hiszen semmi más megkötés nincs a világon, akármilyen \\(k\\)-változós függvény szóba jöhet, semmit nem mondtunk a függvényformáról, összeadhatjuk a változókat, összeszorozhatjuk, hatványozhatjuk, bármilyen mveletet végezhetünk, bármilyen konstanst belerakhatunk, és az összes ilyen közül mondjuk meg, hogy ez a kifejezés mikor lesz a legkisebb?! Az érdekes az, hogy bármilyen abszurdan is néz ki, a dolognak van megoldása! Ráadásul a végeredmény nem is túl bonyolult: \\(f\\) legjobb megválasztása adott pontban \\(Y\\) feltételes várható értéke lesz az kérdéses pontban: \\[ f_{\\text{opt}}\\left(\\mathbf{x}\\right)=\\mathbb{E}\\left(Y \\mid \\underline{X}=\\mathbf{x}\\right) \\] Bizonyítsuk is be ezt! Legyen \\(f_{\\text{opt}}\\) a feltételes várható érték, \\(f\\) pedig egy tetszleges \\(k\\)-változós függvényt. Alakítsuk át a kritériumfüggvényt: \\[\\begin{align*} \\mathbb{E}\\left[Y-f\\left(\\underline{X}\\right)\\right]^2&amp;=\\mathbb{E}\\left[Y-f_{\\text{opt}}\\left(\\underline{X}\\right)+f_{\\text{opt}}\\left(\\underline{X}\\right)-f\\left(\\underline{X}\\right)\\right]^2=\\\\ &amp;=\\mathbb{E}\\left[Y-f_{\\text{opt}}\\left(\\underline{X}\\right)\\right]^2+\\mathbb{E}\\left\\{\\left[Y-f_{\\text{opt}}\\left(\\underline{X}\\right)\\right]\\left[f_{\\text{opt}}\\left(\\underline{X}\\right)-f\\left(\\underline{X}\\right)\\right]\\right\\}+\\\\ &amp;+\\mathbb{E}\\left[f_{\\text{opt}}\\left(\\underline{X}\\right)-f\\left(\\underline{X}\\right)\\right]^2. \\end{align*}\\] A középs tag szerencsére nulla, ezt toronyszabállyal láthatjuk be: \\[\\begin{align*} &amp;\\mathbb{E}\\left\\{\\left[Y-f_{\\text{opt}}\\left(\\underline{X}\\right)\\right]\\left[f_{\\text{opt}}\\left(\\underline{X}\\right)-f\\left(\\underline{X}\\right)\\right]\\right\\}=\\\\ &amp;=\\mathbb{E}\\left\\{\\mathbb{E}\\left\\{\\left[Y-f_{\\text{opt}}\\left(\\underline{X}\\right)\\right]\\left[f_{\\text{opt}}\\left(\\underline{X}\\right)-f\\left(\\underline{X}\\right)\\right] \\right\\} \\mid \\underline{X} \\right\\}=\\\\ &amp;=\\mathbb{E}\\left\\{\\left[f_{\\text{opt}}\\left(\\underline{X}\\right)-f_{\\text{opt}}\\left(\\underline{X}\\right)\\right]\\mathbb{E}\\left[f_{\\text{opt}}\\left(\\underline{X}\\right)-f\\left(\\underline{X}\\right)\\right] \\mid \\underline{X} \\right\\}=0, \\end{align*}\\] így azt kaptuk, hogy \\[ \\mathbb{E}\\left[Y-f\\left(\\underline{X}\\right)\\right]^2=\\mathbb{E}\\left[Y-f_{\\text{opt}}\\left(\\underline{X}\\right)\\right]^2+\\mathbb{E}\\left[f_{\\text{opt}}\\left(\\underline{X}\\right)-f\\left(\\underline{X}\\right)\\right]^2, \\] amibl már csakugyan látható, hogy \\(f_{\\text{opt}}\\) a legjobb választás, hiszen az els tagra nincsen ráhatásunk (mi ugye \\(f\\)-et állítjuk), a második tag pedig egy négyzet várható értéke, így \\(0\\)-nál kisebb nem lehet, de az csakugyan elérhet, ha \\(f\\)-nek \\(f_{\\text{opt}}\\)-ot választjuk. Látható tehát, hogy ez az eredmény teljesen univerzális, semmit nem tételeztünk fel \\(f\\)-rl! Talán nem felesleges feleleveníteni ezen a pontok a feltételes várható érték fogalmát. A kiindulópont a feltételes eloszlás, amit úgy kapunk, hogy fogjuk az együttes eloszlást, és egy adott ponton (ami a feltétel) átmetszük. Mondjuk legyen a feltétel az, hogy \\(X_1=0,\\!055\\): Az együttes srségfüggvény, ne feledjük, egy hegy (aminek a szintvonalait mutatja az ábra), tehát arról van szó, hogy fogunk egy nagy kést, és a piros vonal mentén végigvágjuk a hegyet. Így ezt kapjuk: Vigyázat, ez még nem srségfüggvény, hiszen nem 1 a görbe alatti területe! De már majdnem megvagyunk, nincs más feladatunk, mint átnormálni (elosztani alkalmas konstanssal), hogy 1 legyen a görbe alatti terület, ez az alkalmas konstans persze a jelenlegi görbe alatti területe lesz, ami nem más, mint a vetületi eloszlás értéke a feltétel pontjában. Elvégezve ezt kapjuk a feltételes eloszlást: A feltételes várható érték nem más, mint a feltételes eloszlás várható érték  tehát ennek a fenti függvénynek a várható érték. Bejelölve rajta: A feltételes várható érték tehát nagyjából 657, és ne feledjük, ez ahhoz a feltételhez tartozik, hogy a tanár:diák arány értéke 0,055. Ahogy az elbb megállapítottuk: ha valaki azt kérdezi, hogy ekkora tanár:diák arány mellett mi a legjobb tippünk a teszteredményre, akkor válaszoljunk 657-et! Ezzel is hibázhatunk persze, de így is ekkor járunk a legjobban (elfogadva persze, hogy négyzetes hibázást minimalizálunk). Jelöljük is be ezt az értéket az eredeti ábránkon: Így ni: ha 0,055-ben kérdeznek meg minket, akkor ez a legjobb tippünk. De az ember itt már vérszemet kap: vajon mi történik, ha kiszámoljuk az összes többi pontban is, hogy mi a legjobb tippünk, tehát a feltételes várhatóértéket?! Íme: Nem lehet nem észrevenni: ezek mind egy egyenesre6 illeszkednek! A dolog természetesen nem véletlen, és azért van így, mert az eloszlás többváltozós normális volt. Ez esetben az optimális sokasági regressziófüggvény csakugyan mindig lineáris, ez tételként kimondható7: ha \\(Y\\) és \\(\\underline{X}\\) együttes eloszlása normális8, akkor \\[ \\mathbb{E}\\left(Y \\mid \\underline{X}\\right)=\\mathbb{E}Y+\\mathbf{C}_{Y\\underline{X}}\\mathbf{C}_{\\underline{X}\\underline{X}}^{-1}\\left(\\underline{X}-\\mathbb{E}\\underline{X}\\right). \\] Egy pillanatra álljunk meg. Eddig feltételes eloszlást csak úgy írtunk, hogy a feltétel az egy konkrét érték (szám vagy vektor) volt: \\(\\mathbb{E}\\left(Y \\mid \\underline{X}=\\mathbf{x}\\right)\\). De itt valami más szerepel! A magyarázathoz elevenítsünk fel egy valszám definíciót: a \\(\\mathbb{E}\\left(Y \\mid \\underline{X}=\\mathbf{x}\\right)\\) egy \\(h\\) transzformációt definiál (hiszen adott \\(\\mathbf{x}\\)-hez hozzárendel egy valós számot), és \\(\\mathbb{E}\\left(Y \\mid \\underline{X}\\right)\\) alatt \\(h\\left(\\underline{X}\\right)\\)-et értjük. Tehát van értelme az \\(\\mathbb{E}\\left(Y \\mid \\underline{X}\\right)\\) objektumnak is, és ez egy valószínségi változó lesz. Számunkra ebbl annyi fontos, hogy ha \\(\\mathbb{E}\\left(Y \\mid \\underline{X}\\right)\\)-t látunk, azt értsük úgy, mint valami, ami minden \\(\\mathbf{x}\\) esetén mködik, bármikor beírható, hogy így \\(\\mathbb{E}\\left(Y \\mid \\underline{X}=\\mathbf{x}\\right)\\) legyen belle. Természetesen ez fontos, hogy ha egy egyenletben szerepel, akkor ezt az átírást mindenhol megtegyük, pl. írhatjuk, hogy \\(\\mathbb{E}\\left(Y \\mid \\underline{X}=\\mathbf{x}\\right)=\\mathbb{E}Y+\\mathbf{C}_{Y\\underline{X}}\\mathbf{C}_{\\underline{X}\\underline{X}}^{-1}\\left(\\underline{X}-\\mathbf{x}\\right)\\) (hiszen \\(\\mathbf{x}\\) várható értéke saját maga). Ez a jelölés tehát egyfajta általánosítás. Egy tulajdonságát már ennyi alapján is rögtön láthatjuk a sokasági regressziófuggvénynek: hogy átmegy a várhatóértékek pontján. Hiszen ha a magyarázó változók értéke épp a várhatóértékük, akkor a második tag kiesik, és azt kapjuk, hogy a regressziófüggvény pont az eredményváltozó várható értékét veszi fel. Visszatérve, ha bevezetjük a \\[ \\beta_0=\\mathbb{E}Y-\\mathbf{C}_{Y\\underline{X}}\\mathbf{C}_{\\underline{X}\\underline{X}}^{-1}\\mathbb{E}\\underline{X} \\] és a \\[ \\begin{pmatrix}\\beta_1&amp;\\beta_2&amp; \\cdots &amp; \\beta_k\\end{pmatrix}^T=\\mathbf{C}_{Y\\underline{X}}\\mathbf{C}_{\\underline{X}\\underline{X}}^{-1}\\underline{X} \\] jelöléseket, akkor írhatjuk, hogy \\[ \\mathbb{E}\\left(Y \\mid \\underline{X}\\right)=\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\ldots + \\beta_k X_k. \\] Ebbl talán még világosabban látszik a korábbi állítás: hogy többváltozós normális eloszlásnál speciálisan a regressziófüggvény lineáris lesz. Érdemes megnézni, hogy a még áttekinthet kétváltozós (\\(Y\\),\\(X_1=X\\)) esetben ez az általános eredmény mire specializálódik: ekkor azt kapjuk, hogy \\(\\mathbb{E}\\left(Y \\mid X\\right)=\\mathbb{E}Y+\\frac{\\mathrm{cov}\\left(X,Y\\right)}{\\mathbb{D}^2 X}\\cdot\\left(X_1-\\mathbb{E}X\\right)\\). Két dolgot vegyünk észre: Korreláció megjelenése: \\[ \\mathbb{E}\\left(Y \\mid X\\right)=\\mathbb{E}Y+\\frac{\\mathrm{cov}\\left(X,Y\\right)}{\\mathbb{D}^2 X}\\left(X-\\mathbb{E}X\\right)=\\mathbb{E}Y+\\frac{\\mathbb{D}Y}{\\mathbb{D} X}\\cdot\\mathrm{corr}\\left(X,Y\\right)\\cdot\\left(X-\\mathbb{E}X\\right). \\] A linearitás megjelenése itt: \\[ \\mathbb{E}\\left(Y \\mid X\\right)=\\mathbb{E}Y+\\frac{\\mathrm{cov}\\left(X,Y\\right)}{\\mathbb{D}^2 X}\\left(X-\\mathbb{E}X\\right)=\\left(\\mathbb{E}Y-\\frac{\\mathrm{cov}\\left(X,Y\\right)}{\\mathbb{D}^2 X}\\cdot \\mathbb{E} X\\right) + X\\cdot \\frac{\\mathrm{cov}\\left(X,Y\\right)}{\\mathbb{D}^2 X}, \\] azaz \\(\\mathbb{E}\\left(Y \\mid X\\right)=\\beta_0 + \\beta_1 X\\), ha \\(\\beta_0=\\left(\\mathbb{E}Y-\\frac{\\mathrm{cov}\\left(X,Y\\right)}{\\mathbb{D}^2 X}\\cdot \\mathbb{E} X\\right)\\) és \\(\\beta_1=\\frac{\\mathrm{cov}\\left(X,Y\\right)}{\\mathbb{D}^2 X}\\). És most egy borzasztó fontos dolog következik. Rakjuk össze a puzzle darabjait: egyfell tudjuk, hogy \\(Y=f\\left(X_1,X_2,\\ldots,X_k\\right)+\\varepsilon\\), másrészt most már azt is megállapítottuk, hogy az itt szerepl \\(f\\) legjobb értéke \\(\\mathbb{E}\\left(Y \\mid \\underline{X}\\right)\\), és ez mindig9 igaz. Tehát azt kaptuk, hogy: \\[ Y=\\mathbb{E}\\left(Y \\mid \\underline{X}\\right)+\\varepsilon \\] Ez a dekompozíciót szokás a regresszió ,,hibaalakjának (error form) nevezni. Lényegében arról van szó, hogy szétbontjuk az eredményváltozó alakulását egy ,,magyarázóváltozókkal elérhet legjobb becslés (már láttuk: a feltételes várhatóérték) és egy ,,maradék hiba részre (ami marad). A regresszióanalízis a feltételes eloszlásra koncentrál! Ezért elvileg olyasmit kéne írnunk, hogy ,,\\(\\left(Y \\mid X\\right) = \\mathbb{E}\\left(Y \\mid \\underline{X}\\right)+\\varepsilon\\), de ezt nem tesszük (az \\(\\left(Y \\mid \\underline{X}\\right)\\) objektumot nem szokás definiálni), ehelyett a bal oldalra simán \\(Y\\)-t írunk (de ne feledjük, hogy ez feltételes). Nagyon fontos látni, hogy a regresszió mindig felírható így! És ráadásul ez biztosan optimális felírás. A mi választásunk az lesz, hogy majd \\(\\mathbb{E}\\left(Y \\mid \\underline{X}\\right)\\) helyébe mit írunk be, például ha tudjuk hogy minden változó együttes eloszlása többváltozós normális, akkor azt, hogy \\(\\mathbb{E}\\left(Y \\mid \\underline{X}\\right)=\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\ldots + \\beta_k X_k\\), és így azt kapjuk, hogy \\[ Y=\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\ldots + \\beta_k X_k+\\varepsilon, \\] de vigyázat, ez már  szemben az elz formulával  nem univerzális, csak normalitás esetén érvényes. Lehetne \\(\\mathbb{E}\\left(Y \\mid \\underline{X}\\right)\\) helyébe más is beírni, de mindaddig, amíg jellegre ilyen megoldást használunk, tehát megadjuk a függvény formáját, csak egy vagy több paraméter az, ami meghatározza a konkrét függvényt, szokás paraméteres regresszióról beszélni. Ez nem kötelez, lehetne az \\(\\mathbb{E}\\left(Y \\mid \\underline{X}\\right)\\) anélkül próbálni közelíteni, hogy bármilyen konkrét függvényforma mellett elkötelezdnék, ekkor beszélünk nem-paraméteres regresszióról. Ilyenekkel most nem foglalkozunk, csak az érzékeltetés kedvéért egy lehetség: TODO Tegyünk még egy megállapítást, ami most nem tnik nagyon izgalmasnak, de a késbbiekben rettent fontos lesz. Annyit kell tudnunk hozzá, hogy a feltételes várható érték is lineáris, valamint, hogyha valaminek kétszer vesszük a várható értékét, az ugyanaz mintha egyszer vennénk, és ez nem csak a szokásos várható értékre, hanem a feltételes várható értékre is igaz: \\[ \\mathbb{E}\\left(\\varepsilon \\mid \\underline{X}\\right)=\\mathbb{E}\\left(Y-\\mathbb{E}\\left(Y \\mid \\underline{X}\\right) \\mid \\underline{X}\\right)=\\mathbb{E}\\left(Y\\mid \\underline{X}\\right) - \\mathbb{E}\\left[\\mathbb{E}\\left(Y\\mid \\underline{X}\\right)\\mid \\underline{X}\\right]=\\mathbb{E}\\left(Y\\mid \\underline{X}\\right) - \\mathbb{E}\\left(Y\\mid \\underline{X}\\right)=0. \\] Azaz azt kapjuk, hogy \\(\\mathbb{E}\\left(\\varepsilon \\mid \\underline{X}\\right)=0\\). Nagyon fontos, hogy értsük, hogy most mit mondunk: ha (!) tényleg  valóságban helyes  \\(\\mathbb{E}\\left(Y \\mid \\underline{X}\\right)\\)-t használjuk, akkor \\(\\mathbb{E}\\left(\\varepsilon \\mid \\underline{X}\\right)=0\\) kell legyen. Ez azért lesz izgalmas, mert \\(\\mathbb{E}\\left(Y \\mid \\underline{X}\\right)\\)-t mi sem tudhatjuk biztosan, majd be kell valamit írnunk a helyébe (például azt, hogy \\(\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\ldots + \\beta_k X_k\\)). Ez a megállapítás tehát azt mondja, hogy ha beletrafálunk a dologba, akkor \\(\\mathbb{E}\\left(\\varepsilon \\mid \\underline{X}\\right)=0\\) kell legyen. De ha nem (például ezt írjuk be, csak épp közben nem normális az eloszlás), akkor már ez egyáltalán nem biztos, hogy igaz lesz! Összefoglalva, ott tartunk, hogy ha az eloszlás normális akkor \\(\\mathbb{E}\\left(Y \\mid \\underline{X}\\right)=\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\ldots + \\beta_k X_k\\) és így \\(Y=\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\ldots + \\beta_k X_k+\\varepsilon\\). A bökken persze ott van, hogy azt mi magunk sem tudhatjuk, hogy milyen a változóink eloszlása. És itt jön egy fontos döntés. Munkánkat úgy fogjuk megkezdeni, hogy azt mondjuk akármilyen is az eloszlás, mi mindenképp az \\(Y=\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\ldots + \\beta_k X_k+\\varepsilon\\) modellt használjuk! Ezt fogjuk lineáris regressziónak nevezni. Még egyszer: ez egy helyes döntés, ha normális a változóink eloszlása, de különben nem. Ha nem ismerjük a változóink eloszlását, akkor ez többé már nem egy matematikailag levezethet szükségszerség, hanem egy választás a részünkrl. De több ok szól e választás mellett: Többváltozós normalitásnál egzaktan ez a helyzet Más esetekben ugyan nem, de cserében nagyon kellemesek a tulajdonságai, különösen ami az interpretációt illeti Az is elmondható, hogy  a Taylor-sorfejtés logikáját követve  bármi más is a jó függvényforma, legalábbis lokálisan ez is jó közelítés kell legyen Bár els ránézésre ez vegytisztán lineáris, valójában majd látni fogjuk, hogy egy sor nemlineáris modell is visszavezethet erre a modellre És végezetül egy lényeges szempont: lesznek majd eszközeink arra, hogy észrevegyük, hogy rossz volt ez a választás, és megpróbáljuk kijavítani De újfent nagyon fontos hangsúlyozni, hogy a valós munka során, ahol nem tudjuk mik az eloszlások, azt, hogy az \\(Y=\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\ldots + \\beta_k X_k+\\varepsilon\\) fennáll, már nem kezelhetjük matematikai szükségszerségnek, hanem mint feltételt fel kell tennünk! 1.6 A szálak összeérnek Ezen a ponton összeérnek a szálak. Vegyük csak újra az elbbi alakot (és feltételezzük, hogy a szükséges feltevések teljesültek): \\[ Y=\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\ldots + \\beta_k X_k+\\varepsilon. \\] Mi itt Az adatok igazából nem osztály-szintek, hanem körzetenkénti átlagok, de ez minket, a mostani kérdésünk szempontjából nem érint, így a továbbiakban az egyszerség kedvéért osztályt fogok mondani. Pedig még össze sem foglaltam a fentieket úgy, hogy a korreláció nem implikál kauzalitást! Egyszeren úgy választottam a paramétereket, hogy megfeleljen a kalifornaiai példának Úgy, hogy az ellipszis középpontját a várhatóérték-vektor adja meg, a tengelyek a kovariancia-mátrix sajátvektorainak irányába mutatnak, féltengelyeik hossza pedig a kovariancia-mátrix megfelel sajátértékeivel arányos. A statisztikusokról szóló viccekkel szemben. Érdemes megfigyelni (ez kétváltozós esetben jó szemmértékkel még érzékelhet vizuálisan is), hogy a regressziófüggvény nem az ellipszisek nagytengelye  tehát a korrelációs mátrix megfelel sajátvektora  irányába mutat! Hanem az ellipszis ,,vízszintesen széls pontjain megy át.) A bizonyítást itt elhagyom, lásd például: Bolla-Krámli: Statisztikai következtetések elmélete. Typotex, 2005. 207-208.oldal. Jelölje \\(\\mathbf{C}_{\\underline{X}\\underline{X}}\\) az \\(X\\)-ek szokásos kovarianciamátrixát, \\(\\mathbf{C}_{Y\\underline{X}}\\) pedig azt az oszlopvektort, amely sorban az összes \\(X\\) kovarianciáját tartalmazza \\(Y\\)-nal. Ha szigorúak akarunk lenni, akkor azért annyit hozzá kell tennünk, hogy ha létezik egyáltalán a feltételes várható érték. Vannak eloszlások, amiknek egyszeren nem létezik várható értéke, úgyhogy ez elvileg nem mindegy, de mi most ilyen helyzetekkel nem fogunk foglalkozni. "],["regresszió-a-mintában-következtetés.html", "2 . fejezet Regresszió a mintában: következtetés 2.1 A hagyományos legkisebb négyzetek (OLS) elve 2.2 Lineáris regresszió becslése OLS-elven", " 2 . fejezet Regresszió a mintában: következtetés \\(\\DeclareMathOperator*{\\argmin}{arg\\,min}\\) \\(\\DeclareMathOperator*{\\argmax}{arg\\,max}\\) \\(\\DeclareMathOperator*{\\rank}{rank}\\) \\(\\def\\uuline#1{\\underline{\\underline{#1}}}\\) Pár fogalmat talán érdemes feleleveníteni következtet statisztikából. Az alapprobléma: a halmaz amire a kérdésünk irányul, a sokaság sajnos azonban ennek minden elemét nem tudjuk megfigyelni (azaz lemérni), csak egy részét, a kisebb részhalmaz a minta. Ez elfordulhat akkor, ha a sokaság TODO 2.1 A hagyományos legkisebb négyzetek (OLS) elve Ilyen becslési elv a hagyományos legkisebb négyzetek (ordinary least squares, OLS) elve. Mint általános becslési el, nem kell hozzá semmilyen regresszió, a legközönségesebb következtet statisztikai példán is elmondható. Példaként vegyük az egyik legelemibb kérdést: sokasági várható érték becslése normalitás esetén, tehát a sokaság eloszlása normális (az egyszerség kedvéért legyen a szórás is ismert, tehát azt nem kell becsülnünk). Ami fontos: bár egy alap következtet statisztika kurzuson nem szokták mondani, de lényegében itt is az a helyzet, hogy egy modellt feltételezünk a sokaságra, jelesül: \\(Y \\sim \\mathcal{N}\\left(\\mu,\\sigma_0^2\\right)\\), amit nem mellesleg úgy is írhatnánk, hogy \\(Y=\\mu+\\varepsilon\\), ahol \\(\\varepsilon\\sim\\mathcal{N}\\left(0,\\sigma_0^2\\right)\\). Most \\(\\mu\\) megbecslése céljából veszünk egy \\(n\\) elem fae (független, azonos eloszlású) mintát a sokaságból; ekkor feltevésünk szerint \\(Y_i=\\mu+\\varepsilon_i\\) lesz az \\(i\\)-edik mintaelem. (A feltevésünk igazából azt jelentette, hogy az \\(\\varepsilon_i\\) változók függetlenek és azonos eloszlásúak). Figyeljünk oda a kis és nagybetkre! A nagy bet valószínségi változó, valami aminek eloszlása van, sokasági dolog. Kisbet egy konkrét szám, nem valószínségi, nincsen eloszlása, mintabeli dolog. Most valaki megkérdezhetné, hogy oké, azt értem, hogy \\(Y\\) miért nagy bet, de az \\(Y_i\\) miért az? Hiszen azt mondtuk, hogy az az egyik mintaelem! Talán a legjobban úgy lehet ezt elképzelni, hogy a véletlen mintavétel az, hogy megkeverjük az urnát, hogy kihúzzunk belle egy golyót. Megáll a keverés, nyúlunk bele az urnába, hogy húzzunk: ekkor számunkra az még egy véletlen dolog, hogy mi lesz az elsként húzott elem, annak eloszlása van (fae mintavétel esetén  tehát ha a golyókat mindig visszadobjuk, és az urnát mindig jól átkeverjük  ugyanaz, mint a sokaság, tehát mint az egész urna eloszlása). Ekkor ez még \\(Y_1\\) számunkra. Ekkor kihúzzuk a golyót, és meglátjuk a konkrét értéket: ez lesz \\(y_1\\). Kicsit matematikusabban szólva: kaptunk egy realizációt \\(Y_1\\)-bl, ez lesz az \\(y_1\\). A másik ami fontos: a modellbl következik egy becsült érték minden mintabeli elemhez, jelen esetben, ha \\(m\\) egy feltételezett érték az ismeretlen sokasági várható értékre, akkor \\[ \\widehat{y_i}=m. \\] (Itt persze elvileg beszélni kellene arról, hogy még ha tudjuk is, hogy a sokasági várható érték \\(m\\), miért pont az lesz a becslésünk is minden mintaelemre. Fogadjuk el intuitíve, egyébként olyan érvelést használhatnánk mint az elz fejezetben, úgy, hogy az egyetlen magyarázó változónk az \\(X_1=1\\).) Egy kis kitér megjegyzés: ha jobban megnézzük a fentieket, akkor láthatjuk, hogy az OLS-elv alkalmazásához igazából nem is kell, hogy a sokasági eloszlást ismerjük, csak annyi a fontos, hogy legyen egy modellünk, és belle tudjunk becsült értékeket származtatni a ténylegesen is ismert megfigyelésekhez. És akkor jöhet az OLS-elv! Egy mondatban összefoglalva: az ismeretlen sokasági paraméterre az a becsült érték, amely mellett a tényleges mintabeli értékek, és az adott paraméter melletti, modellbl származó becsült értékek közti eltérések négyzetének összege a legkisebb! A megoldandó  optimalizációs jelleg  feladat tehát matematikailag: \\[ \\widehat{\\mu}=\\argmin_m \\sum_{i=1}^n \\left(y_i-\\widehat{y_i}\\right)^2=\\argmin_m \\sum_{i=1}^n \\left(y_i-m\\right)^2 \\] És ennek megoldása természetesen \\(\\widehat{\\mu}=\\frac{1}{n}\\sum_{i=1}^n y_i=\\overline{y}\\) ebben a példában. Egyetlen kiegészítést kell tenni a fentiekhez. Megkaptunk ugyan a becslt, csakhogy az \\(\\overline{y}\\) egyetlen konkrét szám. (Hát persze, mert egy konkrét mintához, a \\(\\left\\{y_1,y_2,\\ldots,y_n\\right\\}\\) mintához  kisbetk!  tartozik.) Minket azonban alapvet fontossággal fog érdekelni a becsl mintavételi eloszlása, tehát, hogy ha újra meg újra mintát veszünk ugyanabból a sokaságból, és mindegyik mintából kiszámoljuk a becslfüggvény értékét (jelen esetben a mintaátlagot), akkor annak mi lesz az eloszlása. A becslfüggvényünk az igazából egy transzformáció a mintaelemekkel (,,add össze ket és oszd el a mintanagysággal), de ha egyszer ez a transzformáció megvan, azt nyugodtan ráereszthetjük valószínségi változókra is, nem csak számokra! Ami magyarán azt fogja jelenteni, hogy felírjuk ugyanazt  csak épp kisbetk helyett nagybetkkel. Jelen példában a becslfüggvényünk: \\(\\frac{1}{n}\\sum_{i=1}^n Y_i=\\overline{Y}\\), és íme, ennek már nagyon is eloszlása van, hiszen egy valószínségi változó maga is  ez az eloszlás lesz a mintavételi eloszlás. Megvizsgálhatóak a tulajdonságai, megnézhetjük, hogy a várható értéke egyezik-e a sokasági paraméterrel (torzítatlanság), hogy mekkora a szórása (hatásosság), hogy hogyan viselkedik, ha \\(n\\) egyre nagyobb (konzisztencia) és így tovább. 2.2 Lineáris regresszió becslése OLS-elven Most vegyük el a lineáris regressziónkat! (Ahol ezt közszemérem-sértés veszélye nélkül megtehetjük.) Azt látjuk, hogy ott eddig a sokaságról beszéltünk, feltettünk egy modellt (ugyanúgy mint az elbbi példában), jó, lehet, hogy egy kicsit bonyolultabbat, de akkor is, ugyanúgy egy sokaságra vonatkozó modell, amibl, megint csak pontosan ugyanúgy mint az elbbi példában, tudunk egy becsült értéket elállítani minden mintaelemhez. Ez lehetvé teszi, hogy az ismeretlen paramétereket OLS-elven megbecsüljük! Lássuk a részleteket. A változóink az \\(\\left(Y,X_1,X_2,\\ldots,X_k\\right)\\), ezekre vegyünk egy \\(n\\) elem mintát; az \\(i\\)-edik mintaelemet jelölje \\(\\left(Y_i,X_{i1},X_{i2},\\ldots,X_{ik}\\right)\\). Természetesen a modellünk ezekre is igaz lesz, tehát írhatjuk, hogy \\[ Y_i = \\beta_0 + \\beta_1 X_{i1} + \\beta_2 X_{i2} + \\ldots + \\beta_k X_{ik} + \\varepsilon_i. \\] Ez minden \\(i\\)-re teljesül, tehát ha nagyon elszántak vagyunk, akkor \\(n\\) ilyen egyenletet írhatnánk fel: \\[\\begin{align*} Y_1 &amp;= \\beta_0 + \\beta_1 X_{11} + \\beta_2 X_{12} + \\ldots + \\beta_k X_{1k} + \\varepsilon_1 \\\\ Y_2 &amp;= \\beta_0 + \\beta_1 X_{21} + \\beta_2 X_{22} + \\ldots + \\beta_k X_{2k} + \\varepsilon_2 \\\\ \\ldots \\\\ Y_n &amp;= \\beta_0 + \\beta_1 X_{n1} + \\beta_2 X_{n2} + \\ldots + \\beta_k X_{nk} + \\varepsilon_n \\\\ \\end{align*}\\] Az \\(i\\)-edik mintaelem realizációja az \\(\\left(y_i,x_{i1},x_{i2},\\ldots,x_{ik}\\right)\\). (A minta egyelre legyen fae  hogy ez mennyire jó feltevés, arról késbb még fogunk beszélni.) Ha \\(b_0, b_1, \\ldots, b_k\\)-val jelöljük a feltételezett sokasági paramétereket, akkor a becslés \\[ \\widehat{y_i}=b_0 + b_1 x_{i1} + b_2 x_{i2} + \\ldots + b_k x_{ik} \\] lesz az \\(i\\)-edik mintaelemre. (Itt szerencsére nincs mit gondolkozni, hiszen azt az elz fejezetben részletesen levezettük, hogy ez lesz a legjobb becslés adott \\(\\mathbf{x}\\) mellett.) Most hogy megvannak a becsült értékek (\\(\\widehat{y_i}\\)) és a tényleges értékek (\\(y_i\\)), bet szerint ugyanazt az optimalizációs feladatot kell felírnunk, mint az elbb, csak \\(\\widehat{y_i}\\) lesz kicsit hosszabb, ha kifejtjük: \\[\\begin{align*} &amp;\\left(\\widehat{\\beta_0},\\widehat{\\beta_1},\\widehat{\\beta_2},\\ldots,\\widehat{\\beta_k}\\right)=\\argmin_{b_0,b_1,b_2,\\ldots,b_k} \\sum_{i=1}^n \\left(y_i-\\widehat{y_i}\\right)^2=\\\\ &amp;=\\argmin_{b_0,b_1,b_2,\\ldots,b_k} \\sum_{i=1}^n \\left[y_i-\\left(b_0 + b_1 x_{i1} + b_2 x_{i2} + \\ldots + b_k x_{ik}\\right)\\right]^2 \\end{align*}\\] Annyi bonyolódottság van, hogy itt most több paramétert kell becsülni, de ez csak a kivitelezést nehezíti, elvileg teljesen ugyanaz a feladat. Össze ne keverjük \\(\\beta_i\\)-t, \\(b_i\\)-t és \\(\\widehat{\\beta_i}\\)-t! \\(\\beta_i\\) a kérdéses sokasági paraméter valódi, tényleges értéke, egy adott, konkrét szám (csak mi nem tudjuk mennyi), \\(b_i\\) egy általunk feltélezett érték rá, mi állítjuk be, választhatunk nagy számot is, kis számot is, tetszés szerint, a fenti optimalizációban végig fogunk vele futni az összes lehetséges értékén, \\(\\widehat{\\beta_i}\\) pedig a megoldásként kapott legjobb tippünk \\(\\beta_i\\)-re, de ettl még csak tipp, azaz eloszlása lesz, hiszen a mintától is függeni fog, mintáról mintára ingadozni fog (miközben a valódi érték ugyebár állandó  ez lesz a mintavételi hiba forrása). Ezt az optimalizációs problémát kell tehát most megoldanunk. Ezt megtehetnénk a fenti formában is, de célszerbb, ha már most áttérünk a vektoros/mátrixos jelölésekre. Ez eleinte kicsit kényelmetlennek tnhet, de a magasabb absztrakciós szint késbb ki fog fizetdni: lehet, hogy most kicsit nehezebben indulunk, de a cserében a bonyolultabb problémák sem lesznek sokkal nehezebbek. Fogjunk tehát össze mindent értelemszer vektorokba és mátrixokba! A jelölésrendszer teljes bemutatása végett felírom a mintavétel eltti  valószínségi változós  és a realizálódott értékes alakokat is10. Az eredményváltozók: \\[ \\mathbf{y}=\\begin{pmatrix}y_1\\\\y_2\\\\ \\cdots \\\\ y_n\\end{pmatrix}, \\underline{Y}=\\begin{pmatrix}Y_1\\\\Y_2\\\\ \\cdots \\\\ Y_n\\end{pmatrix} \\] A magyarázó változókat nyilván mátrixba kell összefogni, de itt egy kis cselre lesz szükségünk: hozzáveszünk az elejéhez egy csupa 1 oszlopot. (Az így kapott mátrixot a regresszió design mátrixának szokás nevezni.) Íme: \\[ \\mathbf{X}=\\begin{pmatrix}1&amp;x_{11}&amp; x_{12}&amp; \\cdots&amp; x_{1k}\\\\1&amp;x_{21} &amp;x_{22}&amp; \\cdots &amp;x_{2k}\\\\ \\vdots&amp;\\vdots&amp; \\vdots &amp;\\ddots &amp;\\vdots \\\\ 1&amp; x_{n1}&amp; x_{n2}&amp; \\cdots &amp;x_{nk}\\end{pmatrix}, \\uuline{X}=\\begin{pmatrix}1&amp;X_{11}&amp; X_{12}&amp; \\cdots&amp; X_{1k}\\\\1&amp;X_{21} &amp;X_{22}&amp; \\cdots &amp;X_{2k}\\\\ \\vdots&amp;\\vdots&amp; \\vdots &amp;\\ddots &amp;\\vdots \\\\ 1&amp; X_{n1}&amp; X_{n2}&amp; \\cdots &amp;X_{nk}\\end{pmatrix} \\] Ez a csupa 1 oszlop azért lesz célszer, mert ha a regressziós koefficienseket egy \\[ \\pmb{\\beta}=\\begin{pmatrix}\\beta_0\\\\ \\beta_1\\\\ \\cdots \\\\ \\beta_k\\end{pmatrix} \\] vektorba, a hibatagokbat pedig egy \\[ \\pmb{\\varepsilon}=\\begin{pmatrix}\\varepsilon_1\\\\ \\varepsilon_1\\\\ \\cdots \\\\ \\varepsilon_k\\end{pmatrix} \\] vektorba fogjuk össze, akkor a korábbi, \\(n\\) darab egyenletbl álló, igencsak terjengs felírás helyett nemes egyszerséggel ezt írhatjuk: \\[ \\underline{Y}=\\uuline{X} \\pmb{\\beta} + \\pmb{\\varepsilon}. \\] És ennyi, pontosan ugyanaz van leírva! Látható tehát, hogy a csupa 1 oszlop azért kellett, hogy a vektorral való rászorzásnál az legyen a \\(\\beta_0\\) szorzója, így az egyenletben tényleg egyszeren \\(\\beta_0\\) fog megjelenni. Menjünk most vissza az OLS optimalizációs problémájára! Ezekkel a jelölésekkel a kezünkben ugyanis azt is sokkal egyszerbben felírhatjuk: \\[ \\argmin_{\\mathbf{b}} \\widehat{\\mathbf{y}}^T \\widehat{\\mathbf{y}} = \\argmin_{\\mathbf{b}} \\left(\\mathbf{y}-\\mathbf{X}\\mathbf{b}\\right)^T \\left(\\mathbf{y}-\\mathbf{X}\\mathbf{b}\\right), \\] hiszen számok négyzetösszegét megkapjuk, ha összefogjuk ket egy vektorba, és vesszük ezen vektor saját transzponáltjával vett szorzatát. (\\(\\widehat{\\mathbf{y}}\\) és \\(\\mathbf{b}\\) az értelemszer vektorok, \\(\\widehat{y_i}\\)-ket és \\(b_i\\)-ket fogják össze.) Az \\(\\left(\\mathbf{y}-\\mathbf{X}\\mathbf{b}\\right)^T \\left(\\mathbf{y}-\\mathbf{X}\\mathbf{b}\\right)\\) hibanégyzetösszeget \\(ESS\\)-sel (error sum of squares) is fogjuk jelölni11. És akkor essünk neki: oldjuk meg ezt az optimalizációt! Elször alakítsuk át a célfüggvényt, bontsuk fel a zárójeleket: \\[ \\argmin_{\\mathbf{b}} \\left(\\mathbf{y}-\\mathbf{X}\\mathbf{b}\\right)^T\\left(\\mathbf{y}-\\mathbf{X}\\mathbf{b}\\right)=\\argmin_{\\mathbf{b}} \\left[\\mathbf{y}^T \\mathbf{y}-2\\mathbf{b}^T\\mathbf{X}^T\\mathbf{y}+\\mathbf{b}^T\\mathbf{X}^T\\mathbf{X}\\mathbf{b}\\right]. \\] Itt egyszer algebrai átalakításokat végzünk (és a definíciókat használjuk), hiszen a zárójeleket felbontani, mveleteket elvégezni, mátrixokkal/vektorokkal is hasonlóan kell mint valós számokkal. (A transzponálás tagonként elvégezhet, azaz \\(\\left(\\mathbf{a}-\\mathbf{b}\\right)^T=\\mathbf{a}^T-\\mathbf{b}^T\\).) Egyedül annyit kell észrevenni, hogy a \\(\\mathbf{y}^T\\mathbf{X}\\mathbf{b}\\) egy egyszer valós szám, ezért megegyezik a saját transzponáltjával, \\(\\mathbf{b}^T\\mathbf{X}^T\\mathbf{y}\\)-nal. Ezért írhattunk \\(-\\left(\\mathbf{X}\\mathbf{b}\\right)^T\\mathbf{y}-\\mathbf{y}^T\\mathbf{X}\\mathbf{b}\\) helyett egyszeren  például  \\(-2\\mathbf{b}^T\\mathbf{X}^T\\mathbf{y}\\)-t. (Itt mindenhol felhasználtuk, hogy a transzponálás megfordítja a szorzás sorrendjét: \\(\\left(\\mathbf{A}\\mathbf{B}\\right)^T=\\mathbf{A}^T\\mathbf{B}^T\\).) Most jön a minimum megkeresése. Az ember rávágja, hogy deriválni kell, de itt ez picit zrsebb, hiszen a függvényünk többváltozós (ráadásul az is határozatlan, hogy pontosan hányváltozós). Itt jelentkezik igazán a mátrixos jelölésrendszer elnye. A \\(\\mathbf{y}^T \\mathbf{y}-2\\mathbf{y}^T\\mathbf{X}\\mathbf{b}+\\mathbf{b}^T\\mathbf{X}^T\\mathbf{X}\\mathbf{b}\\) lényegében egy ,,másodfokú kifejezés többváltozós értelemben (az \\(ax^2+bx+c\\) többváltozós megfelelje), és ami igazán szép: pont ahogy az \\(ax^2+bx+c\\) lederiválható a változója (\\(x\\)) szerint (eredmény \\(2ax+b\\)), ugyanúgy ez is lederiválható a változója (azaz \\(\\mathbf{b}\\)) szerint és az eredmény az egyváltozóssal teljesen analóg lesz, ahogy fent is látható! (Ez persze bizonyítást igényel!  lásd többváltozós analízisbl.) Bár ezzel átléptünk egyváltozóról többváltozóra, a többváltozós analízisbeli eredmények biztosítanak róla, hogy formálisan ugyanúgy végezhet el a deriválás. (Ezt írja le röviden a ,,vektor szerinti deriválás jelölése. Egy \\(\\mathbf{b}\\) vektor szerinti derivált alatt azt a vektort értjük, melyet úgy kapunk, hogy a deriválandó kifejezést lederiváljuk \\(\\mathbf{b}\\) egyes \\(b_i\\) komponensei szerint  ez ugye egyszer skalár szerinti deriválás, ami már definiált! , majd ez eredményeket összefogjuk egy vektorba. Látható tehát, hogy a vektor szerinti derivált egy ugyanolyan dimenziós vektor, mint ami szerint deriváltunk.) Ami igazán erteljes ebben az eredményben, az nem is egyszeren az, hogy ,,több változónk van, hanem, hogy nem is kell tudnunk, hogy mennyi  mégis, általában is mködik! Az eredmény tehát: \\[\\begin{align*} &amp;\\frac{\\partial}{\\partial \\mathbf{b}} \\left[\\mathbf{y}^T \\mathbf{y}-2\\mathbf{b}^T\\mathbf{X}^T\\mathbf{y}+\\mathbf{b}^T\\mathbf{X}^T\\mathbf{X}\\mathbf{b}\\right]=\\\\ &amp;=-2\\mathbf{X}^T\\mathbf{y}+2\\mathbf{X}^T\\mathbf{X}\\mathbf{b}=0 \\Rightarrow \\widehat{\\pmb{\\beta}_{\\mathrm{OLS}}}=\\left(\\mathbf{X}^T\\mathbf{X}\\right)^{-1}\\mathbf{X}^T\\mathbf{y}, \\end{align*}\\] ha \\(\\mathbf{X}^T\\mathbf{X}\\) nem szinguláris. Azt, hogy a megtalált stacionaritási pont tényleg minimumhely, úgy ellenrizhetjük, hogy megvizsgáljuk a Hesse-mátrixot a pontban. A mátrixos jelölésrendszerben ennek az elállítása is egyszer, még egyszer deriválni kell a függvényt a változó(vektor) szerint: \\[ \\frac{\\partial^2}{\\partial \\mathbf{b}^2} \\left[\\mathbf{y}^T \\mathbf{y}-2\\mathbf{b}^T\\mathbf{X}^T\\mathbf{y}+\\mathbf{b}^T\\mathbf{X}^T\\mathbf{X}\\mathbf{b}\\right] = \\frac{\\partial}{\\partial \\mathbf{b}} \\left[ -2\\mathbf{X}^T\\mathbf{y}+2\\mathbf{X}^T\\mathbf{X}\\mathbf{b} \\right]= 2\\mathbf{X}^T\\mathbf{X}. \\] Az ismert tétel szerint a függvénynek akkor van egy pontban ténylegesen is (lokális, de a konvexitás miatt egyben globális) minimuma, ha ott a Hesse-mátrix pozitív definit. Esetünkben ez minden pontban teljesül. A \\(\\mathbf{X}^T\\mathbf{X}\\) ugyanis pozitív szemidefinit (ez egy skalárszorzat-mátrix, más néven Gram-mátrix, amelyek mindig pozitív szemidefinitek), a kérdés tehát csak a határozott definitség. Belátható azonban, hogy ennek feltétele, hogy \\(\\mathbf{X}^T\\mathbf{X}\\) ne legyen szinguláris  azaz itt is ugyanahhoz a feltételhez értünk! Megjegyezzük, hogy ez pontosan akkor valósul meg, ha az \\(\\mathbf{X}\\) teljes oszloprangú. (Erre a kérdésre a modellfeltevések tárgyalásakor még visszatérünk.) Végül egy számítástechnikai megjegyzés: az együtthatók számításánál a fenti formula direkt követése általában nem a legjobb út, különösen ha sok megfigyelési egység és/vagy változó van. Ekkor nagyméret mátrixot kéne invertálni, amit numerikus okokból (kerekítési hibák, numerikus instabilitás stb.) általában nem szeretünk. Ehelyett, a különféle programok igyekeznek a direkt mátrixinverziót elkerülni, tipikusan az \\(\\mathbf{X}\\) valamilyen célszer mátrix dekompozíciójával (QR-dekompozíció, Cholesky-dekompozíció). Extrém esetekben még az is elképzelhet, hogy az egzakt, zárt alakú megoldás elállítása helyett valamilyen iteratív optimalizálási algoritmus (gradiens módszer, NewtonRaphson-módszer) alkalmazása a gyakorlatban járható út, annak ellenére is, hogy elvileg van zárt alakban megoldása. A kapott eredmény nem más, mintha \\(\\mathbf{X}\\) Moore-Penrose pszeudoinverzével szoroznánk \\(\\mathbf{y}\\)-t. TODO Végezzük el a fenti mveleteket közvetlenül lekódolva R-ben a már látott kaliforniai iskolás példára, ha a pontszámot a tanár:diák arányt a pontszámmal és a jövedelemmel regresszáljuk: y &lt;- CASchools$score X &lt;- cbind( 1, CASchools$tsratio, CASchools$income ) solve( t(X)%*%X )%*%t(X)%*%y ## [,1] ## [1,] 614.0 ## [2,] 233.4 ## [3,] 1.8 Egy mátrixot a t függvénnyel transzponálhatunk és a solve függvénnyel invertálhatunk, a cbind pedig vektorokat, mint oszlopvektorokat fz egybe mátrixszá. (Valaki megkérdezheti, hogy akkor az 1 miért mködik, hiszen az nem vektor: ez az R egyik jellemz  kétél fegyverként viselked  tulajdonsága: megengedi a trehányságot, ugyanis érzékeli, hogy mi a helyzet, és automatikusan egymás alá rakja annyiszor, mint amilyen hosszúak a többi vektorok.) Természetesen az R tartalmaz beépített parancsot regressziók becslésére: lm( score ~ tsratio + income, data = CASchools) ## ## Call: ## lm(formula = score ~ tsratio + income, data = CASchools) ## ## Coefficients: ## (Intercept) tsratio income ## 613.98 233.41 1.84 Az lm a lineáris modell rövidítése. Els argumentumban a regressziós egyenletet kell megadnunk, mint egy R formula (tehát ~ felel meg az egyenlségjelnek, bal oldalán az eredményváltozó, jobb oldalán a magyarázó változók felsorolása, + jellel elválasztva.) Az R konstans alapbeállításként rak a modellbe, azt kell külön kérnünk ha nem szeretnénk (egy -1 hozzáfzésével az utolsó magyarázó változó után). A data argumentum tartalma a szokásos: ha használjuk, akkor a formulában elég a változóneveket leírni, nem kell jelölni, hogy melyik adatkeretre vonatkoznak, mert az R úgy érti, hogy mind a data argumentumban megadottra értend. A jelölésrendszer sajnos nem tökéletesen konzisztens, hiszen \\(\\mathbf{X}\\) nagybet, és mégis kisbets dolgokat fog össze. Nem akartam szakítani a lineáris algebra hagyományával, hogy a mátrixot nagybet jelöli, bár ez tényleg keveredik a valószínségszámítás nagybetjével. Abból azonban, hogy vastagítás vagy aláhúzás van, mindenképpen világos lesz, hogy valószínségi változóról vagy realizálódott értékrl van szó, még ha a kis és nagy bet nem is segít. Sajnos néhány irodalom az általunk használt \\(ESS\\)-re inkább az \\(RSS\\)-t (residual sum of squares) rövidítést használja, ami a jelölési zavarok legszerencsétlenebb típusa, ugyanis az \\(RSS\\)-t majd késbb mi is fogjuk használni, csak épp másra. Éppen ezért, ha ilyenekrl olvasunk, mindig tisztázni kell, hogy a könyv vagy program írói mit értenek alatta. "],["kategoriális-magyarázó-változók.html", "3 . fejezet Kategoriális magyarázó változók 3.1 Regresszió csak minségi változóval (ANOVA) 3.2 Regresszió minségi és mennyiségi magyarázó változóval (ANCOVA)", " 3 . fejezet Kategoriális magyarázó változók 3.1 Regresszió csak minségi változóval (ANOVA) 3.1.1 Minségi változók a regresszióban A kérdés, ami mostani kutatásainkat motiválja: hogyan szerepeltethetünk egy (nominális vagy ordinális, szokás kategoriális változónak is nevezni) tulajdonságot, pl. férfin, egészégesbeteg, alapfokúközépfokúfelsfokú végzettség stb. egy regressziós modellben A regresszió csak számszer adatokat tud felhasználni \\(\\rightarrow\\) valahogy kell a kategoriális tulajdonság lehetséges értékeit (kimeneteit, csoportjait) Eddig csak mennyiségi tulajdonságokkal foglalkoztunk, aminek kódolása triviális volt: a naturáliában kifejezett értékével (m\\(^2\\), eFt stb.) Pl. férfi = 0, n = 1 elég kézenfekv, de mi van az iskolai végzettséggel? Az alap = 0, közép = 1, fels = 2 belekódolja az adatokba, hogy a fels és a közép közti különbség ugyanakkora lenni, mint a közép és alap közötti (ha fels = 3, akkor kétszer akkora stb.) De mi semmi ilyet nem akarunk, hiszen azt szeretnénk, hogy ezt az adatok mondják meg! 3.1.2 Dummy változó fogalma A kódolást megvalósíthatjuk olyan változóval vagy változókkal, melyek 0 vagy 1 értéket vehetnek fel Az ilyen változókat nevezzük dummy (bináris vagy indikátor) változónak Ha két kimenet van, akkor a kódolás teljesen kézenfekv: egy dummy változóra van szükségünk, mely (például) 0 értéket vesz fel férfira, 1-et nre Bonyolultabb a helyzet, ha több kimenet van Triviális kódolás: \\(D_A\\) \\(D_B\\) \\(D_C\\) A 1 0 0 B 0 1 0 C 0 0 1 3.1.3 Kódolás Ezen ,,kódolási tábla alapján a kódolás (pl. \\(X_1\\): jövedelem, \\(X_2\\): iskolai végzettség, \\(X_3\\): életkor): \\[ \\begin{array}{cc} &amp; \\begin{array}{cccc} \\phantom{X_0}&amp;X_1 &amp; X_2 &amp; X_3 \\\\ \\end{array} \\\\ &amp;\\left[ \\begin{array}{cccc} 1 &amp; 213 &amp; B &amp; 32 \\\\ 1 &amp; 311 &amp; C &amp; 41 \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots \\\\ 1 &amp; 128 &amp; B &amp; 18\\\\ \\end{array} \\right] \\end{array} \\rightarrow \\begin{array}{cc} &amp; \\begin{array}{cccccc} &amp;X_1 &amp; D_A &amp; D_B &amp; D_C &amp; \\phantom{0}X_3 \\\\ \\end{array} \\\\ &amp;\\left[ \\begin{array}{cccccc} 1 &amp; 213 &amp; \\phantom{0}0\\phantom{0} &amp; \\phantom{0}1\\phantom{0} &amp; 0\\phantom{0} &amp; 32\\phantom{0} \\\\ 1 &amp; 311 &amp; 0 &amp; 0 &amp; 1\\phantom{0} &amp; 41\\phantom{0} \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots\\phantom{0}&amp; \\vdots\\phantom{0} \\\\ 1 &amp; 128 &amp; 0 &amp; 1 &amp; 0\\phantom{0} &amp; 18\\phantom{0}\\\\ \\end{array} \\right] \\end{array} \\] Itt már minden tisztán numerikus, mködhet a regresszió 3.1.4 Referencia-kódolás ám vegyük észre, hogy 3 csoporthoz kell 3 dummy változó, kódolható 2-vel is! Általában \\(k\\) kimenet kódolása megoldható \\(k-1\\) dummy változóval az ún. referencia-kódolás logikájával Itt kiválasztunk egy kimenetet, aminél mind a \\(k-1\\) darab dummy változó 0 értéket vesz fel (kontrollcsoport vagy referenciacsoport), és a többi \\(k-1\\) csoportot az jelzi, hogy a \\(k-1\\) dummy változó közül vesz fel 1 értéket (mindig csak 1!) Például (3 kimenetre): \\(R_A\\) \\(R_B\\) A 1 0 B 0 1 C 0 0 Itt C a referenciacsoport, \\(R_A\\) és \\(R_B\\) a két szükséges (ugye \\(k=3\\)!) magyarázó változó Vegyük észre, hogy \\(R_A \\equiv D_A\\) és \\(R_B \\equiv D_B\\) (tehát a két kódoláshoz pontosan ugyanazon dummykra van szükség, csak a referencia-kódolásnál eldobjuk az egyiket  ez lesz a kontrollcsoport) 3.1.5 Dummy változó csapda Ha van konstans a modellben, akkor is \\(k\\) csoporthoz \\(k\\) dummyt használni a kódoláshoz Ellenkez esetben egzakt multikollinearitás jön létre (gondoljuk végig, hogy a dummy változókhoz mi tartozik a design mátrixban, ld. elbb!); ez az ún. Ha \\(k\\) csoportot mégis \\(k\\) dummyval kódolunk (,,triviális kódolás), akkor viszont nem szerepeltethetünk konstanst 3.1.6 Triviális kódolás konstans nélkül A két kódolási mód (\\(k\\) darab dummy, nincs konstans és \\(k-1\\) darab dummy, van konstans) jól szemléltethet egy csak a nominális tulajdonsággal magyarázó regresszióval \\(k\\) darab dummy, nincs konstans: \\(D_A\\) \\(D_B\\) \\(D_C\\) A 1 0 0 B 0 1 0 C 0 0 1 \\[ Y=\\beta_A D_A + \\beta_B D_B + \\beta_C D_C + \\varepsilon \\] - Együtthatók értelmezése: ha az \\(A\\) csoportban vagyunk, akkor a fenti egyenlet \\(Y=\\beta_A+\\varepsilon\\) lesz \\(\\Rightarrow\\) \\(\\beta_A\\) az \\(A\\) csoport csoportátlaga (legkisebb négyzetes elv!); hasonlóan a többi 3.1.7 Referencia-kódolás konstanssal \\(k-1\\) darab dummy, van konstans: \\(D_A\\) \\(D_B\\) A 1 0 B 0 1 C 0 0 \\[ Y=\\beta^* + \\beta_A^* D_A + \\beta_B^* D_B + \\varepsilon \\] 3.1.8 Együtthatók értelmezése referencia-kódolásnál Értelmezésnél egy dolgot tartsunk mindig szem eltt: ugyanarra a csoportra ugyanannak az értéknek kell kijönnie, akárhogy kódolunk! Így \\(\\beta_C=\\beta^*\\) Továbbá (a \\(B\\) csoport példáján): \\[ \\beta_B=\\beta^* + \\beta_B^*=\\beta_C + \\beta_B^* \\Rightarrow \\beta_B^* = \\beta_B-\\beta_C \\] Tehát az együtthatók az jelentik a referenciacsoporttól (ami pedig a konstansba kerül) Vegyük észre, hogy a változónkénti szignifikanciák eltérhetnek (mert másra fognak vonatkozni), de az elrejelzése  és így a modellminsít mutatók  nem 3.1.9 Fontos hipotézisvizsgálatok Egyrészt: szignifikáns-e egy adott csoport átlagának eltérése a referenciacsoport átlagától Ez itt nem más, mint \\(\\beta^*_A\\) vagy \\(\\beta^*_B\\) relevanciája Egyszeren \\(t\\)-próbával ellenrizhet! Másrészt: van-e egyáltalán bármilyen csoportok közötti eltérés: \\[\\begin{align*} H_0&amp;: \\beta_A^*=\\beta_B^*=\\ldots=0\\\\ H_1&amp;: \\exists j: \\beta_j^*\\neq 0 \\end{align*}\\] Több csoport átlaga eltér-e? De hát az az ANOVA! Az egyezés nem pusztán formai, teljes tartalmazi egyezés van (ez nem csak hasonló, hanem ugyanaz: az ANOVA elmondása regressziós ,,keretben) 3.1.10 Egynél több kategoriális magyarázó változó Ha egynél több kategoriális magyarázó változó van, akkor nem kódolható mindegyik triviálisan, ilyenkor már a konstans eltávolítása sem segít (Nem az lesz a baj, hogy valamelyik összege a konstans, hanem, hogy a kett összege ugyanaz  ez elvileg is megoldhatatlan) Referencia-kódolás minden további nélkül használható A kétszempontos ANOVA megfelelje regressziós keretben! Természetesen feltételezhet interakció is, ez esetben a dummy-kat az összes lehetséges kombinációban szorozni kell 3.2 Regresszió minségi és mennyiségi magyarázó változóval (ANCOVA) 3.2.1 Dummyzás folytonos magyarázó változó jelenléte mellett} Amit eddig csináltunk az lényegében az volt, amit nevezhetünk: csoportonként eltér (de konstans) értékkel becsültük az eredményváltozót Mi van, ha bevonunk egy magyarázó változót? Azaz ekkor már nem egy konstanst becsülünk az egyes csoportokra, hanem egy egyenest (a folytonos magyarázó változó függvényében) Dummyzással (tehát a csoporttagság szerint) eltéríthetjük az egyenesek tengelymetszetét és meredekségét is! Lehet csoportonként különböz +1 egység magyarázó változó hatása a 0 magyarázó változóhoz tartozó eredményváltozó E feladat neve: ANCOVA 3.2.2 Eltér tengelymetszet Ha csak a tengelymetszetet térítjük el (+1 egység magyarázó változó hatása ugyanaz minden csoportban, de nem ugyanannyi a 0 magyarázó változóhoz tartozó eredményváltozó): Algebrailag: \\[ %Y=\\beta^* + \\beta_A^* D_A + \\beta_B^* D_B + \\beta_X X + u Y=\\beta_0 + \\beta_D D + \\beta_X X + \\varepsilon \\] 3.2.3 Eltér meredekség Ha csak a meredekséget térítjük el (0 magyarázó változóhoz ugyanaz az eredményváltozó tartozik, de +1 egység magyarázó változó hatása csoportonként eltér): Algebrailag: \\[ %Y=\\beta_1 + \\left(\\beta_X + \\beta_{X,A}^* D_A + \\beta_{X,B}^* D_B\\right) X + u Y=\\beta_0 + \\left(\\beta_X + \\beta_D D\\right) X + \\varepsilon \\] 3.2.4 Eltér tengelymetszet és meredekség Akár a tengelymetszet a meredekség is lehet különböz Ahogy elbb láttuk, csak a módszereket kell kombinálni: a konstanst és a meredekséget is megdummyzzuk: \\[ Y=\\beta_1 + \\beta_2 X + \\varepsilon, \\] de úgy, hogy \\(\\beta_1=\\alpha + \\alpha_A D_A + \\alpha_B D_B\\) és \\(\\beta_2=\\gamma+ \\gamma_A D_A + \\gamma_B D_B\\) Nagyon fontos észrevenni, hogy a meredekség dummyzása a dummy és a mennyiségi változó közti interakcióra vezet: \\[ Y=\\alpha + \\alpha_A D_A + \\alpha_B D_B+ \\gamma X + \\gamma_A \\left(D_A X\\right) + \\gamma_B \\left(D_B X \\right) + \\varepsilon \\] Logikus is: az egyik változó (folytonos) hatása eltér a szerint, hogy a másik változónak (kategoriális) mi a szintje: különböz meredekségek Avagy fordítva elmondva (egyenértéken, hiszen az interakció ugye szimmetrikus): az egyik változó (kategoriális) hatása eltér a szerint, hogy a másik változónak (folytonos) mi a szintje: az egyenesek közti különbség függ attól, hogy hol nézzük 3.2.5 Eltér tengelymetszet és meredekség De hát ez megoldható a minta szétszedésével is! A két módszer  természetesen  ugyanarra az eredményre vezet A dummyzás mégis jobb a minta szétszedésénél; vajon miért? Mert messzemenen több lehetségünk van a dummyzott (egybenlév) modellel \\(\\rightarrow\\) gazdaságilag releváns hipotézisek vizsgálhatóak egyszeren (ld. mindjárt) 3.2.6 Hipotézisvizsgálat a dummyzott modellben Pl.: van-e egyáltalán bármilyen eltérés a csoportok között? (Értsd: eltér-e a becsült egyenes (bármilyen szempontból) a csoportok között, vagy mindegyikben teljesen ugyanaz?) Ez az ún. , hipotézispárja: \\(H_0: \\alpha_A = \\alpha_B =\\gamma_A = \\gamma_B =0\\), \\(H_1\\): valamelyik ezek közül nem nulla, tehát van strukturális törés És most jön a szép rész: ha a fenti modellt megbecsültük (sima OLS-sel), akkor ez a hipotézis egyszeren egy közönséges Wald- (vagy hasonló) próbát jelent! Hasonlóképp: nem lehet, hogy csak a tengelymetszetek eltérek? \\(\\rightarrow\\) ez az ún. hipotézise, \\(H_0:\\gamma_A = \\gamma_B =0\\); szintén Wald-teszttel elintézhet Minden hasonló, gazdasági kérdés ökonometriailag, például változó vagy változók relevanciájának tesztelésére 3.2.7 Kontraszt-kódolás Kontraszt-kódolás: trükkös kódolás úgy kitalálva, hogy a dummy-k együtthatója ne a referencia-csoporthoz, hanem az átlaghoz képesti eltérést jelentse A megoldás: \\(C_A\\) \\(C_B\\) A 1 0 B 0 1 C -1 -1 (A dummy változó nem 0 és 1 értéket vehet csak fel) Miért fog ez mködni? Mert: \\[\\begin{align} \\beta_0 + \\beta_{C_{A}} + 0 = \\overline{y}_{A} \\\\ \\beta_0 + 0 + \\beta_{C_{B}} = \\overline{y}_{B} \\\\ \\beta_0 - \\beta_{C_{A}} - \\beta_{C_{B}} = \\overline{y}_{C} \\end{align}\\] És így: (1)+(2)+(3) \\(\\Rightarrow\\) \\(3\\beta_0=\\overline{y}_{A}+\\overline{y}_{B}+\\overline{y}_{C}\\) \\(\\Rightarrow\\) \\(\\beta_0\\) tényleg a fátlag (ha azonosak a csoportok elemszámai! különben ún. súlyozott kontraszt kellene, ahol a dummy változók már nem is feltétlenül egész értékeket vennének fel) (2)+(3) \\(\\Rightarrow\\) \\(2\\beta_0-\\beta_{C_{A}}=\\overline{y}_{B}+\\overline{y}_{C} \\Rightarrow \\beta_{C_{A}}=2\\beta_0-\\left(\\overline{y}_{B}+\\overline{y}_{C}\\right)=2\\beta_0-\\left(3\\beta_0-\\overline{y}_{A}\\right) \\Rightarrow \\beta_{C_{A}}=\\overline{y}_{A}-\\beta_0\\) \\(\\Rightarrow\\) tényleg az átlagtól való eltérés (és hasonlóan a másik) 3.2.8 Egy terminológiai megjegyzés Az angol irodalomban az általunk kontrasztkódolásnak nevezett módszert nagyon gyakran ,,effect coding-nak nevezik a kontraszt pedig az, amikor a csoportok tetszleges  általunk meghatározott  lineáris kombinációját teszteljük "]]
