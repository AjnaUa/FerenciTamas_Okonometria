<!DOCTYPE html>
<html lang="hu" xml:lang="hu">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>2 . fejezet Regresszió a mintában: következtetés | Ökonometria</title>
  <meta name="description" content="Az ökonometria a társadalmi-gazdasági jelenségek kvantitatív, empirikus vizsgálatának, modellezésének tudománya. A jegyzet ezt tárgyalja, kerülve a mély matematikai részleteket, a hangsúlyt inkább a módszerek sokféleségének bemutatására helyezve. A számításokat mindenhol R statisztikai környezet alatt szemlélti." />
  <meta name="generator" content="bookdown 0.17 and GitBook 2.6.7" />

  <meta property="og:title" content="2 . fejezet Regresszió a mintában: következtetés | Ökonometria" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://tamas-ferenci.github.io/FerenciTamas_Okonometria/" />
  <meta property="og:image" content="https://tamas-ferenci.github.io/FerenciTamas_Okonometria/FerenciTamas_Okonometria_cover.png" />
  <meta property="og:description" content="Az ökonometria a társadalmi-gazdasági jelenségek kvantitatív, empirikus vizsgálatának, modellezésének tudománya. A jegyzet ezt tárgyalja, kerülve a mély matematikai részleteket, a hangsúlyt inkább a módszerek sokféleségének bemutatására helyezve. A számításokat mindenhol R statisztikai környezet alatt szemlélti." />
  <meta name="github-repo" content="tamas-ferenci/FerenciTamas_Okonometria" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="2 . fejezet Regresszió a mintában: következtetés | Ökonometria" />
  
  <meta name="twitter:description" content="Az ökonometria a társadalmi-gazdasági jelenségek kvantitatív, empirikus vizsgálatának, modellezésének tudománya. A jegyzet ezt tárgyalja, kerülve a mély matematikai részleteket, a hangsúlyt inkább a módszerek sokféleségének bemutatására helyezve. A számításokat mindenhol R statisztikai környezet alatt szemlélti." />
  <meta name="twitter:image" content="https://tamas-ferenci.github.io/FerenciTamas_Okonometria/FerenciTamas_Okonometria_cover.png" />

<meta name="author" content="Ferenci Tamás," />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="út-az-ökonometriához.html"/>

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-19799395-2"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-19799395-2');
</script>


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(title);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Ferenci Tamás: Ökonometria</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Előszó</a></li>
<li class="chapter" data-level="1" data-path="út-az-ökonometriához.html"><a href="út-az-ökonometriához.html"><i class="fa fa-check"></i><b>1</b> Út az ökonometriához</a><ul>
<li class="chapter" data-level="1.1" data-path="út-az-ökonometriához.html"><a href="út-az-ökonometriához.html#történetünk-első-szála-néhány-motiváló-példa"><i class="fa fa-check"></i><b>1.1</b> Történetünk első szála: néhány motiváló példa</a><ul>
<li class="chapter" data-level="1.1.1" data-path="út-az-ökonometriához.html"><a href="út-az-ökonometriához.html#hogyan-hat-az-osztálylétszám-a-tanulók-teljesítményére"><i class="fa fa-check"></i><b>1.1.1</b> Hogyan hat az osztálylétszám a tanulók teljesítményére?</a></li>
<li class="chapter" data-level="1.1.2" data-path="út-az-ökonometriához.html"><a href="út-az-ökonometriához.html#csökkenti-e-a-korrupció-mértékét-a-nők-részvétele-a-politikában"><i class="fa fa-check"></i><b>1.1.2</b> Csökkenti-e a korrupció mértékét a nők részvétele a politikában?</a></li>
<li class="chapter" data-level="1.1.3" data-path="út-az-ökonometriához.html"><a href="út-az-ökonometriához.html#csalnak-e-az-orosz-választásokon"><i class="fa fa-check"></i><b>1.1.3</b> Csalnak-e az orosz választásokon?</a></li>
<li class="chapter" data-level="1.1.4" data-path="út-az-ökonometriához.html"><a href="út-az-ökonometriához.html#további-példák"><i class="fa fa-check"></i><b>1.1.4</b> További példák</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="út-az-ökonometriához.html"><a href="út-az-ökonometriához.html#a-példák-tanulságai-az-empirikus-adatok-elemzésének-legnagyobb-problémája"><i class="fa fa-check"></i><b>1.2</b> A példák tanulságai: az empirikus adatok elemzésének legnagyobb problémája</a></li>
<li class="chapter" data-level="1.3" data-path="út-az-ökonometriához.html"><a href="út-az-ökonometriához.html#a-confounding-megoldásai-kísérlet-és-megfigyelés"><i class="fa fa-check"></i><b>1.3</b> A confounding megoldásai: kísérlet és megfigyelés</a></li>
<li class="chapter" data-level="1.4" data-path="út-az-ökonometriához.html"><a href="út-az-ökonometriához.html#történetünk-második-szála-az-ökonometriai-modellek-és-a-regresszió"><i class="fa fa-check"></i><b>1.4</b> Történetünk második szála: az ökonometriai modellek és a regresszió</a></li>
<li class="chapter" data-level="1.5" data-path="út-az-ökonometriához.html"><a href="út-az-ökonometriához.html#regresszió-a-sokaságban"><i class="fa fa-check"></i><b>1.5</b> Regresszió a sokaságban</a></li>
<li class="chapter" data-level="1.6" data-path="út-az-ökonometriához.html"><a href="út-az-ökonometriához.html#a-szálak-összeérnek"><i class="fa fa-check"></i><b>1.6</b> A szálak összeérnek</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="regresszió-a-mintában-következtetés.html"><a href="regresszió-a-mintában-következtetés.html"><i class="fa fa-check"></i><b>2</b> Regresszió a mintában: következtetés</a><ul>
<li class="chapter" data-level="2.1" data-path="regresszió-a-mintában-következtetés.html"><a href="regresszió-a-mintában-következtetés.html#a-hagyományos-legkisebb-négyzetek-ols-elve"><i class="fa fa-check"></i><b>2.1</b> A hagyományos legkisebb négyzetek (OLS) elve</a></li>
<li class="chapter" data-level="2.2" data-path="regresszió-a-mintában-következtetés.html"><a href="regresszió-a-mintában-következtetés.html#lineáris-regresszió-becslése-ols-elven"><i class="fa fa-check"></i><b>2.2</b> Lineáris regresszió becslése OLS-elven</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="http://www.medstat.hu/" target="blank">http://www.medstat.hu/</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Ökonometria</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="regresszió-a-mintában-következtetés" class="section level1">
<h1><span class="header-section-number">2 . fejezet</span> Regresszió a mintában: következtetés</h1>
<p><span class="math inline">\(\DeclareMathOperator*{\argmin}{arg\,min}\)</span>
<span class="math inline">\(\DeclareMathOperator*{\argmax}{arg\,max}\)</span>
<span class="math inline">\(\DeclareMathOperator*{\rank}{rank}\)</span>
<span class="math inline">\(\def\uuline#1{\underline{\underline{#1}}}\)</span></p>
<p>Pár fogalmat talán érdemes feleleveníteni következtető statisztikából. Az alapprobléma: a halmaz amire a kérdésünk irányul, a <strong>sokaság</strong> sajnos azonban ennek minden elemét nem tudjuk megfigyelni (azaz lemérni), csak egy részét, a kisebb részhalmaz a <strong>minta</strong>. Ez előfordulhat akkor, ha a sokaság TODO</p>
<div id="a-hagyományos-legkisebb-négyzetek-ols-elve" class="section level2">
<h2><span class="header-section-number">2.1</span> A hagyományos legkisebb négyzetek (OLS) elve</h2>
<p>Ilyen becslési elv a hagyományos legkisebb négyzetek (ordinary least squares, OLS) elve. Mint általános becslési el, nem kell hozzá semmilyen regresszió, a legközönségesebb következtető statisztikai példán is elmondható. Példaként vegyük az egyik legelemibb kérdést: sokasági várható érték becslése normalitás esetén, tehát a sokaság eloszlása normális (az egyszerűség kedvéért legyen a szórás is ismert, tehát azt nem kell becsülnünk). Ami fontos: bár egy alap következtető statisztika kurzuson nem szokták mondani, de lényegében itt is az a helyzet, hogy egy <em>modellt</em> feltételezünk a sokaságra, jelesül: <span class="math inline">\(Y \sim \mathcal{N}\left(\mu,\sigma_0^2\right)\)</span>, amit nem mellesleg úgy is írhatnánk, hogy <span class="math inline">\(Y=\mu+\varepsilon\)</span>, ahol <span class="math inline">\(\varepsilon\sim\mathcal{N}\left(0,\sigma_0^2\right)\)</span>. Most <span class="math inline">\(\mu\)</span> megbecslése céljából veszünk egy <span class="math inline">\(n\)</span> elemű fae (független, azonos eloszlású) mintát a sokaságból; ekkor feltevésünk szerint <span class="math inline">\(Y_i=\mu+\varepsilon_i\)</span> lesz az <span class="math inline">\(i\)</span>-edik mintaelem. (A feltevésünk igazából azt jelentette, hogy az <span class="math inline">\(\varepsilon_i\)</span> változók függetlenek és azonos eloszlásúak). Figyeljünk oda a kis és nagybetűkre! A nagy betű valószínűségi változó, valami aminek eloszlása van, sokasági dolog. Kisbetű egy konkrét szám, nem valószínűségi, nincsen eloszlása, mintabeli dolog. Most valaki megkérdezhetné, hogy oké, azt értem, hogy <span class="math inline">\(Y\)</span> miért nagy betű, de az <span class="math inline">\(Y_i\)</span> miért az? Hiszen azt mondtuk, hogy az az egyik mintaelem…! Talán a legjobban úgy lehet ezt elképzelni, hogy a véletlen mintavétel az, hogy megkeverjük az urnát, hogy kihúzzunk belőle egy golyót. Megáll a keverés, nyúlunk bele az urnába, hogy húzzunk: ekkor számunkra az még egy véletlen dolog, hogy mi lesz az elsőként húzott elem, annak eloszlása van (fae mintavétel esetén – tehát ha a golyókat mindig visszadobjuk, és az urnát mindig jól átkeverjük – ugyanaz, mint a sokaság, tehát mint az egész urna eloszlása). Ekkor ez még <span class="math inline">\(Y_1\)</span> számunkra. Ekkor kihúzzuk a golyót, és meglátjuk a konkrét értéket: ez lesz <span class="math inline">\(y_1\)</span>. Kicsit matematikusabban szólva: kaptunk egy realizációt <span class="math inline">\(Y_1\)</span>-ből, ez lesz az <span class="math inline">\(y_1\)</span>.</p>
<p>A másik ami fontos: a modellből következik egy <em>becsült érték</em> minden mintabeli elemhez, jelen esetben, ha <span class="math inline">\(m\)</span> egy feltételezett érték az ismeretlen sokasági várható értékre, akkor
<span class="math display">\[
        \widehat{y_i}=m.
    \]</span>
(Itt persze elvileg beszélni kellene arról, hogy még ha tudjuk is, hogy a sokasági várható érték <span class="math inline">\(m\)</span>, miért pont az lesz a becslésünk is minden mintaelemre. Fogadjuk el intuitíve, egyébként olyan érvelést használhatnánk mint az előző fejezetben, úgy, hogy az egyetlen magyarázó változónk az <span class="math inline">\(X_1=1\)</span>.)</p>
<p>Egy kis kitérő megjegyzés: ha jobban megnézzük a fentieket, akkor láthatjuk, hogy az OLS-elv alkalmazásához igazából nem is kell, hogy a sokasági eloszlást ismerjük, csak annyi a fontos, hogy legyen egy modellünk, és belőle tudjunk becsült értékeket származtatni a ténylegesen is ismert megfigyelésekhez.</p>
<p>És akkor jöhet az OLS-elv! Egy mondatban összefoglalva: az ismeretlen sokasági paraméterre az a becsült érték, amely mellett a tényleges mintabeli értékek, és az adott paraméter melletti, modellből származó becsült értékek közti eltérések négyzetének összege a legkisebb! A megoldandó – optimalizációs jellegű – feladat tehát matematikailag:
<span class="math display">\[
        \widehat{\mu}=\argmin_m \sum_{i=1}^n \left(y_i-\widehat{y_i}\right)^2=\argmin_m \sum_{i=1}^n \left(y_i-m\right)^2
    \]</span>
És ennek megoldása természetesen <span class="math inline">\(\widehat{\mu}=\frac{1}{n}\sum_{i=1}^n y_i=\overline{y}\)</span> ebben a példában.</p>
<p>Egyetlen kiegészítést kell tenni a fentiekhez. Megkaptunk ugyan a becslőt, csakhogy az <span class="math inline">\(\overline{y}\)</span> egyetlen konkrét szám. (Hát persze, mert egy konkrét mintához, a <span class="math inline">\(\left\{y_1,y_2,\ldots,y_n\right\}\)</span> mintához – kisbetűk! – tartozik.) Minket azonban alapvető fontossággal fog érdekelni a becslő <strong>mintavételi eloszlása</strong>, tehát, hogy ha újra meg újra mintát veszünk ugyanabból a sokaságból, és mindegyik mintából kiszámoljuk a becslőfüggvény értékét (jelen esetben a mintaátlagot), akkor annak mi lesz az eloszlása. A becslőfüggvényünk az igazából egy <em>transzformáció</em> a mintaelemekkel (,,add össze őket és oszd el a mintanagysággal’’), de ha egyszer ez a transzformáció megvan, azt nyugodtan ráereszthetjük valószínűségi változókra is, nem csak számokra! Ami magyarán azt fogja jelenteni, hogy felírjuk ugyanazt – csak épp kisbetűk helyett nagybetűkkel. Jelen példában a becslőfüggvényünk: <span class="math inline">\(\frac{1}{n}\sum_{i=1}^n Y_i=\overline{Y}\)</span>, és íme, ennek már nagyon is eloszlása van, hiszen egy valószínűségi változó maga is – ez az eloszlás lesz a mintavételi eloszlás. Megvizsgálhatóak a tulajdonságai, megnézhetjük, hogy a várható értéke egyezik-e a sokasági paraméterrel (torzítatlanság), hogy mekkora a szórása (hatásosság), hogy hogyan viselkedik, ha <span class="math inline">\(n\)</span> egyre nagyobb (konzisztencia) és így tovább.</p>
</div>
<div id="lineáris-regresszió-becslése-ols-elven" class="section level2">
<h2><span class="header-section-number">2.2</span> Lineáris regresszió becslése OLS-elven</h2>
<p>Most vegyük elő a lineáris regressziónkat! (Ahol ezt közszemérem-sértés veszélye nélkül megtehetjük.) Azt látjuk, hogy ott eddig a sokaságról beszéltünk, feltettünk egy modellt (<em>ugyanúgy mint az előbbi példában</em>), jó, lehet, hogy egy kicsit bonyolultabbat, de akkor is, ugyanúgy egy sokaságra vonatkozó modell, amiből, megint csak pontosan ugyanúgy mint az előbbi példában, tudunk egy becsült értéket előállítani minden mintaelemhez. Ez lehetővé teszi, hogy az ismeretlen paramétereket OLS-elven megbecsüljük!</p>
<p>Lássuk a részleteket. A változóink az <span class="math inline">\(\left(Y,X_1,X_2,\ldots,X_k\right)\)</span>, ezekre vegyünk egy <span class="math inline">\(n\)</span> elemű mintát; az <span class="math inline">\(i\)</span>-edik mintaelemet jelölje <span class="math inline">\(\left(Y_i,X_{i1},X_{i2},\ldots,X_{ik}\right)\)</span>. Természetesen a modellünk ezekre is igaz lesz, tehát írhatjuk, hogy
<span class="math display">\[
  Y_i = \beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} + \ldots + \beta_k X_{ik} + \varepsilon_i.
\]</span>
Ez minden <span class="math inline">\(i\)</span>-re teljesül, tehát ha nagyon elszántak vagyunk, akkor <span class="math inline">\(n\)</span> ilyen egyenletet írhatnánk fel:
<span class="math display">\[\begin{align*}
  Y_1 &amp;= \beta_0 + \beta_1 X_{11} + \beta_2 X_{12} + \ldots + \beta_k X_{1k} + \varepsilon_1 \\
  Y_2 &amp;= \beta_0 + \beta_1 X_{21} + \beta_2 X_{22} + \ldots + \beta_k X_{2k} + \varepsilon_2 \\
  \ldots \\
  Y_n &amp;= \beta_0 + \beta_1 X_{n1} + \beta_2 X_{n2} + \ldots + \beta_k X_{nk} + \varepsilon_n \\
\end{align*}\]</span></p>
<p>Az <span class="math inline">\(i\)</span>-edik mintaelem realizációja az <span class="math inline">\(\left(y_i,x_{i1},x_{i2},\ldots,x_{ik}\right)\)</span>. (A minta egyelőre legyen fae – hogy ez mennyire jó feltevés, arról később még fogunk beszélni.)</p>
<p>Ha <span class="math inline">\(b_0, b_1, \ldots, b_k\)</span>-val jelöljük a feltételezett sokasági paramétereket, akkor a becslés
<span class="math display">\[
  \widehat{y_i}=b_0 + b_1 x_{i1} + b_2 x_{i2} + \ldots + b_k x_{ik}
\]</span>
lesz az <span class="math inline">\(i\)</span>-edik mintaelemre. (Itt szerencsére nincs mit gondolkozni, hiszen azt az előző fejezetben részletesen levezettük, hogy ez lesz a legjobb becslés adott <span class="math inline">\(\mathbf{x}\)</span> mellett.)</p>
<p>Most hogy megvannak a becsült értékek (<span class="math inline">\(\widehat{y_i}\)</span>) és a tényleges értékek (<span class="math inline">\(y_i\)</span>), betű szerint ugyanazt az optimalizációs feladatot kell felírnunk, mint az előbb, csak <span class="math inline">\(\widehat{y_i}\)</span> lesz kicsit hosszabb, ha kifejtjük:
<span class="math display">\[\begin{align*}
        &amp;\left(\widehat{\beta_0},\widehat{\beta_1},\widehat{\beta_2},\ldots,\widehat{\beta_k}\right)=\argmin_{b_0,b_1,b_2,\ldots,b_k} \sum_{i=1}^n \left(y_i-\widehat{y_i}\right)^2=\\
        &amp;=\argmin_{b_0,b_1,b_2,\ldots,b_k} \sum_{i=1}^n \left[y_i-\left(b_0 + b_1 x_{i1} + b_2 x_{i2} + \ldots + b_k x_{ik}\right)\right]^2
\end{align*}\]</span>
Annyi bonyolódottság van, hogy itt most <em>több</em> paramétert kell becsülni, de ez csak a kivitelezést nehezíti, elvileg teljesen ugyanaz a feladat.</p>
<p>Össze ne keverjük <span class="math inline">\(\beta_i\)</span>-t, <span class="math inline">\(b_i\)</span>-t és <span class="math inline">\(\widehat{\beta_i}\)</span>-t! <span class="math inline">\(\beta_i\)</span> a kérdéses sokasági paraméter valódi, tényleges értéke, egy adott, konkrét szám (csak mi nem tudjuk mennyi), <span class="math inline">\(b_i\)</span> egy általunk feltélezett érték rá, mi állítjuk be, választhatunk nagy számot is, kis számot is, tetszés szerint, a fenti optimalizációban végig fogunk vele futni az összes lehetséges értékén, <span class="math inline">\(\widehat{\beta_i}\)</span> pedig a megoldásként kapott <em>legjobb tippünk</em> <span class="math inline">\(\beta_i\)</span>-re, de ettől még csak tipp, azaz eloszlása lesz, hiszen a mintától is függeni fog, mintáról mintára ingadozni fog (miközben a valódi érték ugyebár állandó – ez lesz a mintavételi hiba forrása).</p>
<p>Ezt az optimalizációs problémát kell tehát most megoldanunk. Ezt megtehetnénk a fenti formában is, de célszerűbb, ha már most áttérünk a vektoros/mátrixos jelölésekre. Ez eleinte kicsit kényelmetlennek tűnhet, de a magasabb absztrakciós szint később ki fog fizetődni: lehet, hogy most kicsit nehezebben indulunk, de a cserében a bonyolultabb problémák sem lesznek sokkal nehezebbek.</p>
<p>Fogjunk tehát össze mindent értelemszerű vektorokba és mátrixokba! A jelölésrendszer teljes bemutatása végett felírom a mintavétel előtti – valószínűségi változós – és a realizálódott értékes alakokat is<a href="#fn10" class="footnote-ref" id="fnref10"><sup>10</sup></a>. Az eredményváltozók:
<span class="math display">\[
  \mathbf{y}=\begin{pmatrix}y_1\\y_2\\ \cdots \\ y_n\end{pmatrix}, \underline{Y}=\begin{pmatrix}Y_1\\Y_2\\ \cdots \\ Y_n\end{pmatrix}
\]</span>
A magyarázó változókat nyilván mátrixba kell összefogni, de itt egy kis cselre lesz szükségünk: hozzáveszünk az elejéhez egy csupa 1 oszlopot. (Az így kapott mátrixot a regresszió <strong>design mátrixának</strong> szokás nevezni.) Íme:
<span class="math display">\[
  \mathbf{X}=\begin{pmatrix}1&amp;x_{11}&amp; x_{12}&amp; \cdots&amp; x_{1k}\\1&amp;x_{21} &amp;x_{22}&amp; \cdots &amp;x_{2k}\\ \vdots&amp;\vdots&amp; \vdots &amp;\ddots &amp;\vdots \\ 1&amp; x_{n1}&amp; x_{n2}&amp; \cdots &amp;x_{nk}\end{pmatrix}, \uuline{X}=\begin{pmatrix}1&amp;X_{11}&amp; X_{12}&amp; \cdots&amp; X_{1k}\\1&amp;X_{21} &amp;X_{22}&amp; \cdots &amp;X_{2k}\\ \vdots&amp;\vdots&amp; \vdots &amp;\ddots &amp;\vdots \\ 1&amp; X_{n1}&amp; X_{n2}&amp; \cdots &amp;X_{nk}\end{pmatrix}
\]</span>
Ez a csupa 1 oszlop azért lesz célszerű, mert ha a regressziós koefficienseket egy
<span class="math display">\[
  \pmb{\beta}=\begin{pmatrix}\beta_0\\ \beta_1\\ \cdots \\ \beta_k\end{pmatrix}
\]</span>
vektorba, a hibatagokbat pedig egy
<span class="math display">\[
  \pmb{\varepsilon}=\begin{pmatrix}\varepsilon_1\\ \varepsilon_1\\ \cdots \\ \varepsilon_k\end{pmatrix}
\]</span>
vektorba fogjuk össze, akkor a korábbi, <span class="math inline">\(n\)</span> darab egyenletből álló, igencsak terjengős felírás helyett nemes egyszerűséggel ezt írhatjuk:
<span class="math display">\[
  \underline{Y}=\uuline{X} \pmb{\beta} + \pmb{\varepsilon}.
\]</span>
És ennyi, pontosan ugyanaz van leírva!</p>
<p>Látható tehát, hogy a csupa 1 oszlop azért kellett, hogy a vektorral való rászorzásnál az legyen a <span class="math inline">\(\beta_0\)</span> szorzója, így az egyenletben tényleg egyszerűen <span class="math inline">\(\beta_0\)</span> fog megjelenni.</p>
<p>Menjünk most vissza az OLS optimalizációs problémájára! Ezekkel a jelölésekkel a kezünkben ugyanis azt is sokkal egyszerűbben felírhatjuk:
<span class="math display">\[
        \argmin_{\mathbf{b}} \widehat{\mathbf{y}}^T \widehat{\mathbf{y}} = \argmin_{\mathbf{b}} \left(\mathbf{y}-\mathbf{X}\mathbf{b}\right)^T \left(\mathbf{y}-\mathbf{X}\mathbf{b}\right),
\]</span>
hiszen számok négyzetösszegét megkapjuk, ha összefogjuk őket egy vektorba, és vesszük ezen vektor saját transzponáltjával vett szorzatát. (<span class="math inline">\(\widehat{\mathbf{y}}\)</span> és <span class="math inline">\(\mathbf{b}\)</span> az értelemszerű vektorok, <span class="math inline">\(\widehat{y_i}\)</span>-ket és <span class="math inline">\(b_i\)</span>-ket fogják össze.)</p>
<p>Az <span class="math inline">\(\left(\mathbf{y}-\mathbf{X}\mathbf{b}\right)^T \left(\mathbf{y}-\mathbf{X}\mathbf{b}\right)\)</span> hibanégyzetösszeget <span class="math inline">\(ESS\)</span>-sel (error sum of squares) is fogjuk jelölni<a href="#fn11" class="footnote-ref" id="fnref11"><sup>11</sup></a>.</p>
<p>És akkor essünk neki: oldjuk meg ezt az optimalizációt! Először alakítsuk át a célfüggvényt, bontsuk fel a zárójeleket:
<span class="math display">\[
    \argmin_{\mathbf{b}} \left(\mathbf{y}-\mathbf{X}\mathbf{b}\right)^T\left(\mathbf{y}-\mathbf{X}\mathbf{b}\right)=\argmin_{\mathbf{b}} \left[\mathbf{y}^T \mathbf{y}-2\mathbf{b}^T\mathbf{X}^T\mathbf{y}+\mathbf{b}^T\mathbf{X}^T\mathbf{X}\mathbf{b}\right].
\]</span>
Itt egyszerű algebrai átalakításokat végzünk (és a definíciókat használjuk), hiszen a zárójeleket felbontani, műveleteket elvégezni, mátrixokkal/vektorokkal is hasonlóan kell mint valós számokkal. (A transzponálás tagonként elvégezhető, azaz <span class="math inline">\(\left(\mathbf{a}-\mathbf{b}\right)^T=\mathbf{a}^T-\mathbf{b}^T\)</span>.) Egyedül annyit kell észrevenni, hogy a <span class="math inline">\(\mathbf{y}^T\mathbf{X}\mathbf{b}\)</span> egy egyszerű valós szám, ezért megegyezik a saját transzponáltjával, <span class="math inline">\(\mathbf{b}^T\mathbf{X}^T\mathbf{y}\)</span>-nal. Ezért írhattunk <span class="math inline">\(-\left(\mathbf{X}\mathbf{b}\right)^T\mathbf{y}-\mathbf{y}^T\mathbf{X}\mathbf{b}\)</span> helyett egyszerűen – például – <span class="math inline">\(-2\mathbf{b}^T\mathbf{X}^T\mathbf{y}\)</span>-t. (Itt mindenhol felhasználtuk, hogy a transzponálás megfordítja a szorzás sorrendjét: <span class="math inline">\(\left(\mathbf{A}\mathbf{B}\right)^T=\mathbf{A}^T\mathbf{B}^T\)</span>.)</p>
<p>Most jön a minimum megkeresése. Az ember rávágja, hogy deriválni kell, de itt ez picit zűrősebb, hiszen a függvényünk többváltozós (ráadásul az is határozatlan, hogy pontosan hányváltozós). Itt jelentkezik igazán a mátrixos jelölésrendszer előnye. A <span class="math inline">\(\mathbf{y}^T \mathbf{y}-2\mathbf{y}^T\mathbf{X}\mathbf{b}+\mathbf{b}^T\mathbf{X}^T\mathbf{X}\mathbf{b}\)</span> lényegében egy ,,másodfokú kifejezés’’ többváltozós értelemben (az <span class="math inline">\(ax^2+bx+c\)</span> többváltozós megfelelője), és ami igazán szép: pont ahogy az <span class="math inline">\(ax^2+bx+c\)</span> lederiválható a változója (<span class="math inline">\(x\)</span>) szerint (eredmény <span class="math inline">\(2ax+b\)</span>), ugyanúgy ez is lederiválható a változója (azaz <span class="math inline">\(\mathbf{b}\)</span>) szerint… és az eredmény az egyváltozóssal teljesen analóg lesz, ahogy fent is látható! (Ez persze bizonyítást igényel! – lásd többváltozós analízisből.) Bár ezzel átléptünk egyváltozóról többváltozóra, a többváltozós analízisbeli eredmények biztosítanak róla, hogy formálisan ugyanúgy végezhető el a deriválás. (Ezt írja le röviden a ,,vektor szerinti deriválás’’ jelölése. Egy <span class="math inline">\(\mathbf{b}\)</span> vektor szerinti derivált alatt azt a vektort értjük, melyet úgy kapunk, hogy a deriválandó kifejezést lederiváljuk <span class="math inline">\(\mathbf{b}\)</span> egyes <span class="math inline">\(b_i\)</span> komponensei szerint – ez ugye egyszerű skalár szerinti deriválás, ami már definiált! –, majd ez eredményeket összefogjuk egy vektorba. Látható tehát, hogy a vektor szerinti derivált egy ugyanolyan dimenziós vektor, mint ami szerint deriváltunk.) Ami igazán erőteljes ebben az eredményben, az nem is egyszerűen az, hogy ,,több’’ változónk van, hanem, hogy nem is kell tudnunk, hogy mennyi – mégis, általában is működik! Az eredmény tehát:
<span class="math display">\[\begin{align*}
        &amp;\frac{\partial}{\partial \mathbf{b}} \left[\mathbf{y}^T \mathbf{y}-2\mathbf{b}^T\mathbf{X}^T\mathbf{y}+\mathbf{b}^T\mathbf{X}^T\mathbf{X}\mathbf{b}\right]=\\
        &amp;=-2\mathbf{X}^T\mathbf{y}+2\mathbf{X}^T\mathbf{X}\mathbf{b}=0 \Rightarrow \widehat{\pmb{\beta}_{\mathrm{OLS}}}=\left(\mathbf{X}^T\mathbf{X}\right)^{-1}\mathbf{X}^T\mathbf{y},
\end{align*}\]</span>
ha <span class="math inline">\(\mathbf{X}^T\mathbf{X}\)</span> nem szinguláris.</p>
<p>Azt, hogy a megtalált stacionaritási pont tényleg minimumhely, úgy ellenőrizhetjük, hogy megvizsgáljuk a Hesse-mátrixot a pontban. A mátrixos jelölésrendszerben ennek az előállítása is egyszerű, még egyszer deriválni kell a függvényt a változó(vektor) szerint:
<span class="math display">\[
    \frac{\partial^2}{\partial \mathbf{b}^2}  \left[\mathbf{y}^T \mathbf{y}-2\mathbf{b}^T\mathbf{X}^T\mathbf{y}+\mathbf{b}^T\mathbf{X}^T\mathbf{X}\mathbf{b}\right] = \frac{\partial}{\partial \mathbf{b}} \left[ -2\mathbf{X}^T\mathbf{y}+2\mathbf{X}^T\mathbf{X}\mathbf{b} \right]= 2\mathbf{X}^T\mathbf{X}.
\]</span>
Az ismert tétel szerint a függvénynek akkor van egy pontban ténylegesen is (lokális, de a konvexitás miatt egyben globális) minimuma, ha ott a Hesse-mátrix pozitív definit. Esetünkben ez minden pontban teljesül. A <span class="math inline">\(\mathbf{X}^T\mathbf{X}\)</span> ugyanis pozitív szemidefinit (ez egy skalárszorzat-mátrix, más néven Gram-mátrix, amelyek mindig pozitív szemidefinitek), a kérdés tehát csak a határozott definitség. Belátható azonban, hogy ennek feltétele, hogy <span class="math inline">\(\mathbf{X}^T\mathbf{X}\)</span> ne legyen szinguláris – azaz itt is ugyanahhoz a feltételhez értünk! Megjegyezzük, hogy ez pontosan akkor valósul meg, ha az <span class="math inline">\(\mathbf{X}\)</span> teljes oszloprangú. (Erre a kérdésre a modellfeltevések tárgyalásakor még visszatérünk.)</p>
<p>Végül egy számítástechnikai megjegyzés: az együtthatók számításánál a fenti formula direkt követése általában nem a legjobb út, különösen ha sok megfigyelési egység és/vagy változó van. Ekkor nagyméretű mátrixot kéne invertálni, amit numerikus okokból (kerekítési hibák, numerikus instabilitás stb.) általában nem szeretünk. Ehelyett, a különféle programok igyekeznek a direkt mátrixinverziót elkerülni, tipikusan az <span class="math inline">\(\mathbf{X}\)</span> valamilyen célszerű mátrix dekompozíciójával (QR-dekompozíció, Cholesky-dekompozíció). Extrém esetekben még az is elképzelhető, hogy az egzakt, zárt alakú megoldás előállítása helyett valamilyen iteratív optimalizálási algoritmus (gradiens módszer, Newton–Raphson-módszer) alkalmazása a gyakorlatban járható út, annak ellenére is, hogy elvileg van zárt alakban megoldása.</p>
<p>A kapott eredmény nem más, mintha <span class="math inline">\(\mathbf{X}\)</span> Moore-Penrose pszeudoinverzével szoroznánk <span class="math inline">\(\mathbf{y}\)</span>-t.</p>
<p>TODO</p>
<p>Végezzük el a fenti műveleteket közvetlenül lekódolva R-ben a már látott kaliforniai iskolás példára, ha a pontszámot a tanár:diák arányt a pontszámmal és a jövedelemmel regresszáljuk:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2-1" title="1">y &lt;-<span class="st"> </span>CASchools<span class="op">$</span>score</a>
<a class="sourceLine" id="cb2-2" title="2">X &lt;-<span class="st"> </span><span class="kw">cbind</span>( <span class="dv">1</span>, CASchools<span class="op">$</span>tsratio, CASchools<span class="op">$</span>income )</a>
<a class="sourceLine" id="cb2-3" title="3"><span class="kw">solve</span>( <span class="kw">t</span>(X)<span class="op">%*%</span>X )<span class="op">%*%</span><span class="kw">t</span>(X)<span class="op">%*%</span>y</a></code></pre></div>
<pre><code>##       [,1]
## [1,] 614.0
## [2,] 233.4
## [3,]   1.8</code></pre>
<p>Egy mátrixot a <code>t</code> függvénnyel transzponálhatunk és a <code>solve</code> függvénnyel invertálhatunk, a <code>cbind</code> pedig vektorokat, mint oszlopvektorokat fűz egybe mátrixszá. (Valaki megkérdezheti, hogy akkor az <code>1</code> miért működik, hiszen az nem vektor: ez az R egyik jellemző – kétélű fegyverként viselkedő – tulajdonsága: megengedi a trehányságot, ugyanis érzékeli, hogy mi a helyzet, és automatikusan egymás alá rakja annyiszor, mint amilyen hosszúak a többi vektorok.)</p>
<p>Természetesen az R tartalmaz beépített parancsot regressziók becslésére:</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb4-1" title="1"><span class="kw">lm</span>( score <span class="op">~</span><span class="st"> </span>tsratio <span class="op">+</span><span class="st"> </span>income, <span class="dt">data =</span> CASchools)</a></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = score ~ tsratio + income, data = CASchools)
## 
## Coefficients:
## (Intercept)      tsratio       income  
##      613.98       233.41         1.84</code></pre>
<p>Az <code>lm</code> a lineáris modell rövidítése. Első argumentumban a regressziós egyenletet kell megadnunk, mint egy R formula (tehát <code>~</code> felel meg az egyenlőségjelnek, bal oldalán az eredményváltozó, jobb oldalán a magyarázó változók felsorolása, <code>+</code> jellel elválasztva.) Az R konstans alapbeállításként rak a modellbe, azt kell külön kérnünk ha nem szeretnénk (egy <code>-1</code> hozzáfűzésével az utolsó magyarázó változó után). A <code>data</code> argumentum tartalma a szokásos: ha használjuk, akkor a formulában elég a változóneveket leírni, nem kell jelölni, hogy melyik adatkeretre vonatkoznak, mert az R úgy érti, hogy mind a <code>data</code> argumentumban megadottra értendő.</p>

</div>
</div>
















<div class="footnotes">
<hr />
<ol start="10">
<li id="fn10"><p>A jelölésrendszer sajnos nem tökéletesen konzisztens, hiszen <span class="math inline">\(\mathbf{X}\)</span> nagybetű, és mégis kisbetűs dolgokat fog össze. Nem akartam szakítani a lineáris algebra hagyományával, hogy a mátrixot nagybetű jelöli, bár ez tényleg keveredik a valószínűségszámítás nagybetűjével. Abból azonban, hogy vastagítás vagy aláhúzás van, mindenképpen világos lesz, hogy valószínűségi változóról vagy realizálódott értékről van szó, még ha a kis és nagy betű nem is segít.<a href="regresszió-a-mintában-következtetés.html#fnref10" class="footnote-back">↩</a></p></li>
<li id="fn11"><p>Sajnos néhány irodalom az általunk használt <span class="math inline">\(ESS\)</span>-re inkább az <span class="math inline">\(RSS\)</span>-t (residual sum of squares) rövidítést használja, ami a jelölési zavarok legszerencsétlenebb típusa, ugyanis az <span class="math inline">\(RSS\)</span>-t majd később mi is fogjuk használni, csak épp másra. Éppen ezért, ha ilyenekről olvasunk, mindig tisztázni kell, hogy a könyv vagy program írói mit értenek alatta.<a href="regresszió-a-mintában-következtetés.html#fnref11" class="footnote-back">↩</a></p></li>
</ol>
</div>
<!-- Default Statcounter code for Physcon Research
http://research.physcon.uni-obuda.hu/ -->
<script type="text/javascript">
var sc_project=11601191; 
var sc_invisible=1; 
var sc_security="5a06c22d"; 
</script>
<script type="text/javascript"
src="https://www.statcounter.com/counter/counter.js"
async></script>
<noscript><div class="statcounter"><a title="ingyen
webstatisztika" href="https://www.statcounter.hu/"
target="_blank"><img class="statcounter"
src="https://c.statcounter.com/11601191/0/5a06c22d/1/"
alt="ingyen webstatisztika"></a></div></noscript>
<!-- End of Statcounter Code -->
            </section>

          </div>
        </div>
      </div>
<a href="út-az-ökonometriához.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/tamas-ferenci/FerenciTamas_Okonometria/edit/master/02-regresszioamintabankovetkeztetes.Rmd",
"text": "Szerkesztés"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["FerenciTamas_Okonometria.pdf", "FerenciTamas_Okonometria.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
